@article{fischlerRepresentationMatchingPictorial1973,
  title = {The {{Representation}} and {{Matching}} of {{Pictorial Structures}}},
  author = {Fischler, M.A. and Elschlager, R.A.},
  date = {1973-01},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-22},
  number = {1},
  pages = {67--92},
  issn = {1557-9956},
  doi = {10.1109/T-C.1973.223602},
  url = {https://ieeexplore.ieee.org/document/1672195/?arnumber=1672195},
  urldate = {2025-04-03},
  abstract = {The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of "goodness" of matching or detection.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {_st/New,Dynamic programming heuristic optimization picture description picture matching picture processing representation.},
  file = {/Users/maksimploter/Zotero/storage/7D3D26TZ/Fischler and Elschlager - 1973 - The Representation and Matching of Pictorial Structures.pdf;/Users/maksimploter/Zotero/storage/ALZK3YD8/1672195.html}
}

@article{hintonReducingDimensionalityData2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  date = {2006-07-28},
  journaltitle = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1127647},
  url = {https://www.science.org/doi/10.1126/science.1127647},
  urldate = {2025-04-03},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  keywords = {_st/Cited,_st/New,:todo:skim_first},
  file = {/Users/maksimploter/Zotero/storage/BIUQRDPQ/Hinton and Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2024-06-28},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  langid = {english},
  keywords = {_st/Cited,_st/New,:todo:skim_first,Computer science,Mathematics and computing},
  file = {/Users/maksimploter/Zotero/storage/G99AILUJ/LeCun et al. - 2015 - Deep learning.pdf}
}

@inproceedings{girshickRichFeatureHierarchies2014a,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-06},
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  url = {https://ieeexplore.ieee.org/document/6909475},
  urldate = {2025-04-03},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {_st/New,Feature extraction,Object detection,Proposals,Support vector machines,Training,Vectors,Visualization},
  file = {/Users/maksimploter/Zotero/storage/7C92E6PA/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/maksimploter/Zotero/storage/9SRAIGQR/6909475.html}
}



@article{yurtseverSurveyAutonomousDriving2020b,
  title = {A {{Survey}} of {{Autonomous Driving}}: {{Common Practices}} and {{Emerging Technologies}}},
  shorttitle = {A {{Survey}} of {{Autonomous Driving}}},
  author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {58443--58469},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2983149},
  urldate = {2025-03-10},
  abstract = {Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.},
  keywords = {Accidents,automation,Automation,Autonomous vehicles,control,intelligent transportation systems,intelligent vehicles,Planning,Robot sensing systems,robotics,Systems architecture,Task analysis,Vehicle dynamics},
  file = {/Users/maksimploter/Zotero/storage/5Y4IQW4F/Yurtsever et al. - 2020 - A Survey of Autonomous Driving Common Practices a.pdf;/Users/maksimploter/Zotero/storage/WAVQYPIK/9046805.html}
}

@misc{srivastava2016unsupervisedlearningvideorepresentations,
      title={Unsupervised Learning of Video Representations using LSTMs}, 
      author={Nitish Srivastava and Elman Mansimov and Ruslan Salakhutdinov},
      year={2016},
      eprint={1502.04681},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.04681}, 
}

@online{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  date = {2021-06-22},
  eprint = {2103.03206},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.03206},
  urldate = {2024-06-28},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domainspecific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver – a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/maksimploter/Zotero/storage/7SFRYG4J/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf}
}



@unpublished{tamara_munzner_keynote_2012,
	location = {Broad Institute},
	title = {Keynote on Visualization Principles},
	url = {https://www.youtube.com/watch?v=ZgOF8R6YL2U},
	abstract = {Copyright Broad Institute, 2013. All rights reserved.
Tamara Munzner (www.bit.ly/tmunzner) presents very lucid and useful guidelines for creating effective visualizations, including how to correctly rank visual channel types and how to use categorical color constraints. She explains advantages of 2D representation and drawbacks of 3D, immersive, or animated visualizations. She also describes how to create visualizations that reduce the viewer's cognitive load, and how to validate visualizations. This talk was presented at {VIZBI} 2011, an international conference series on visualizing biological data (vizbi.org) funded by {NIH} \& {EMBO}. Slides from the talk are at bit.ly/{nCJM}5U.

For information about data visualization efforts at the Broad Institute, please visit:
http://www.broadinstitute.org/node/1363/},
	author = {Tamara Munzner},
	urldate = {2024-12-05},
	date = {2012},
}

@book{graves_strategic_2012,
	edition = {2nd edition},
	title = {A Strategic Guide to Technical Communication},
	isbn = {978-1-55481-107-6},
	abstract = {A Strategic Guide to Technical Communication incorporates useful and specific strategies for writers, to enable them to create aesthetically appealing and usable technical documentation. These strategies have been developed and tested on a thousand students from a number of different disciplines over twelve years and three institutions. The second edition adds a chapter on business communication, reworks the discussion on technical style, and expands the information on visual communication and ethics into free-standing chapters.  The text is accompanied by a passcode-protected website containing materials for instructors ({PowerPoint} lectures, lesson plans, sample student work, and helpful links).},
	pagetotal = {328},
	publisher = {Broadview Press},
	author = {Graves, Heather and Graves, Roger},
	date = {2012-05-23},
}
