@article{fischlerRepresentationMatchingPictorial1973,
  title = {The {{Representation}} and {{Matching}} of {{Pictorial Structures}}},
  author = {Fischler, M.A. and Elschlager, R.A.},
  date = {1973-01},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-22},
  number = {1},
  pages = {67--92},
  issn = {1557-9956},
  doi = {10.1109/T-C.1973.223602},
  url = {https://ieeexplore.ieee.org/document/1672195/?arnumber=1672195},
  urldate = {2025-04-03},
  abstract = {The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of "goodness" of matching or detection.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {_st/New,Dynamic programming heuristic optimization picture description picture matching picture processing representation.},
  file = {/Users/maksimploter/Zotero/storage/7D3D26TZ/Fischler and Elschlager - 1973 - The Representation and Matching of Pictorial Structures.pdf;/Users/maksimploter/Zotero/storage/ALZK3YD8/1672195.html}
}

@article{hintonReducingDimensionalityData2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  date = {2006-07-28},
  journaltitle = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1127647},
  url = {https://www.science.org/doi/10.1126/science.1127647},
  urldate = {2025-04-03},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  keywords = {_st/Cited,_st/New,:todo:skim_first},
  file = {/Users/maksimploter/Zotero/storage/BIUQRDPQ/Hinton and Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2024-06-28},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  langid = {english},
  keywords = {_st/Cited,_st/New,:todo:skim_first,Computer science,Mathematics and computing},
  file = {/Users/maksimploter/Zotero/storage/G99AILUJ/LeCun et al. - 2015 - Deep learning.pdf}
}

@inproceedings{girshickRichFeatureHierarchies2014a,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-06},
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  url = {https://ieeexplore.ieee.org/document/6909475},
  urldate = {2025-04-03},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {_st/New,Feature extraction,Object detection,Proposals,Support vector machines,Training,Vectors,Visualization},
  file = {/Users/maksimploter/Zotero/storage/7C92E6PA/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/maksimploter/Zotero/storage/9SRAIGQR/6909475.html}
}

@online{hanSeqNMSVideoObject2016,
  title = {Seq-{{NMS}} for {{Video Object Detection}}},
  author = {Han, Wei and Khorrami, Pooya and Paine, Tom Le and Ramachandran, Prajit and Babaeizadeh, Mohammad and Shi, Honghui and Li, Jianan and Yan, Shuicheng and Huang, Thomas S.},
  date = {2016-08-22},
  eprint = {1602.08465},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.08465},
  url = {http://arxiv.org/abs/1602.08465},
  urldate = {2025-03-27},
  abstract = {Video object detection is challenging because objects that are easily detected in one frame may be difficult to detect in another frame within the same clip. Recently, there have been major advances for doing object detection in a single image. These methods typically contain three phases: (i) object proposal generation (ii) object classification and (iii) post-processing. We propose a modification of the post-processing phase that uses high-scoring object detections from nearby frames to boost scores of weaker detections within the same clip. We show that our method obtains superior results to state-of-the-art single image object detection techniques. Our method placed 3rd in the video object detection (VID) task of the ImageNet Large Scale Visual Recognition Challenge 2015 (ILSVRC2015).},
  pubstate = {prepublished},
  keywords = {_st/Cited,Computer Science - Computer Vision and Pattern Recognition,refining detections,video object detection},
  file = {/Users/maksimploter/Zotero/storage/BHMBB4WF/Han et al. - 2016 - Seq-NMS for Video Object Detection.pdf;/Users/maksimploter/Zotero/storage/8HVHZJ8A/1602.html}
}

@article{kangTCNNTubeletsConvolutional2018,
  title = {T-{{CNN}}: {{Tubelets With Convolutional Neural Networks}} for {{Object Detection From Videos}}},
  shorttitle = {T-{{CNN}}},
  author = {Kang, Kai and Li, Hongsheng and Yan, Junjie and Zeng, Xingyu and Yang, Bin and Xiao, Tong and Zhang, Cong and Wang, Zhe and Wang, Ruohui and Wang, Xiaogang and Ouyang, Wanli},
  date = {2018-10},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {10},
  pages = {2896--2907},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2017.2736553},
  url = {https://ieeexplore.ieee.org/document/8003302},
  urldate = {2025-03-27},
  abstract = {The state-of-the-art performance for object detection has been significantly improved over the past two years. Besides the introduction of powerful deep neural networks, such as GoogleNet and VGG, novel object detection frameworks, such as R-CNN and its successors, Fast R-CNN, and Faster R-CNN, play an essential role in improving the state of the art. Despite their effectiveness on still images, those frameworks are not specifically designed for object detection from videos. Temporal and contextual information of videos are not fully investigated and utilized. In this paper, we propose a deep learning framework that incorporates temporal and contextual information from tubelets obtained in videos, which dramatically improves the baseline performance of existing still-image detection frameworks when they are applied to videos. It is called T-CNN, i.e., tubelets with convolutional neueral networks. The proposed framework won newly introduced an object-detection-from-video task with provided data in the ImageNet Large-Scale Visual Recognition Challenge 2015. Code is publicly available at https://github.com/myfavouritekk/T-CNN.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}} for {{Video Technology}}},
  keywords = {_st/Cited,_st/Skimmed,computer vision,Convolutional codes,neural networks,Neural networks,Object detection,Proposals,Target tracking,Training,Videos},
  file = {/Users/maksimploter/Zotero/storage/CHRGRNR9/Kang et al. - 2018 - T-CNN Tubelets With Convolutional Neural Networks for Object Detection From Videos.pdf;/Users/maksimploter/Zotero/storage/B8ZQA63K/8003302.html}
}

@inproceedings{kangObjectDetectionVideo2016,
  title = {Object {{Detection}} from {{Video Tubelets}} with {{Convolutional Neural Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kang, Kai and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
  date = {2016-06},
  eprint = {1604.04053},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {817--825},
  doi = {10.1109/CVPR.2016.95},
  url = {http://arxiv.org/abs/1604.04053},
  urldate = {2025-03-27},
  abstract = {Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (R-CNN)). The lately introduced ImageNet task on object detection from video (VID) brings the object detection task into the video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task.},
  keywords = {_r/3_worth_citing,_r/4_should_cite,_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition,video object detection},
  file = {/Users/maksimploter/Zotero/storage/67B2K9SH/Kang et al. - 2016 - Object Detection from Video Tubelets with Convolutional Neural Networks.pdf;/Users/maksimploter/Zotero/storage/LKSEAZWN/1604.html}
}
@InProceedings{Lu_2017_ICCV,
author = {Lu, Yongyi and Lu, Cewu and Tang, Chi-Keung},
title = { Online Video Object Detection Using Association LSTM},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}
@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation},
  title={Long Short-Term Memory},
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}
}


@misc{srivastava2016unsupervisedlearningvideorepresentations,
      title={Unsupervised Learning of Video Representations using LSTMs}, 
      author={Nitish Srivastava and Elman Mansimov and Ruslan Salakhutdinov},
      year={2016},
      eprint={1502.04681},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.04681}, 
}

@online{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  date = {2021-06-22},
  eprint = {2103.03206},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.03206},
  urldate = {2024-06-28},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domainspecific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver – a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/maksimploter/Zotero/storage/7SFRYG4J/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf}
}



@unpublished{tamara_munzner_keynote_2012,
	location = {Broad Institute},
	title = {Keynote on Visualization Principles},
	url = {https://www.youtube.com/watch?v=ZgOF8R6YL2U},
	abstract = {Copyright Broad Institute, 2013. All rights reserved.
Tamara Munzner (www.bit.ly/tmunzner) presents very lucid and useful guidelines for creating effective visualizations, including how to correctly rank visual channel types and how to use categorical color constraints. She explains advantages of 2D representation and drawbacks of 3D, immersive, or animated visualizations. She also describes how to create visualizations that reduce the viewer's cognitive load, and how to validate visualizations. This talk was presented at {VIZBI} 2011, an international conference series on visualizing biological data (vizbi.org) funded by {NIH} \& {EMBO}. Slides from the talk are at bit.ly/{nCJM}5U.

For information about data visualization efforts at the Broad Institute, please visit:
http://www.broadinstitute.org/node/1363/},
	author = {Tamara Munzner},
	urldate = {2024-12-05},
	date = {2012},
}

@book{graves_strategic_2012,
	edition = {2nd edition},
	title = {A Strategic Guide to Technical Communication},
	isbn = {978-1-55481-107-6},
	abstract = {A Strategic Guide to Technical Communication incorporates useful and specific strategies for writers, to enable them to create aesthetically appealing and usable technical documentation. These strategies have been developed and tested on a thousand students from a number of different disciplines over twelve years and three institutions. The second edition adds a chapter on business communication, reworks the discussion on technical style, and expands the information on visual communication and ethics into free-standing chapters.  The text is accompanied by a passcode-protected website containing materials for instructors ({PowerPoint} lectures, lesson plans, sample student work, and helpful links).},
	pagetotal = {328},
	publisher = {Broadview Press},
	author = {Graves, Heather and Graves, Roger},
	date = {2012-05-23},
}
