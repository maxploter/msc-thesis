%~~~~~~~~~~~~~~~~~~INTRODUCTION~~~~~~~~~~~~~~~~~~

@article{litmanAutonomousVehicleImplementationb,
	title = {Autonomous Vehicle Implementation Predictions: Implications for Transport Planning},
	author = {Litman, Todd},
	journal = {Transportation Research Board 94th Annual MeetingTransportation Research Board},
	year = {2015},
	litmapsId = {172980305}
}


%~~~~~~~~~~~~~~~~~~~BACKGROUND~~~~~~~~~~~~~~~~~~~
@standard{sae:j3016:2021apr,
  organization = {SAE International},
  title        = {Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles},
  number       = {J3016},
  type         = {Surface Vehicle Recommended Practice},
  date         = {2021-04},
  note         = {Revision April 2021, Superseding J3016 JUN2018},
  % Optional: Add version field if preferred over the note
  % version   = {APR2021}, 
  % Optional: Add location if needed by your style
  % location  = {Warrendale, PA},
}

@article{yurtseverSurveyAutonomousDriving2020,
  title = {A {{Survey}} of {{Autonomous Driving}}: {{Common Practices}} and {{Emerging Technologies}}},
  shorttitle = {A {{Survey}} of {{Autonomous Driving}}},
  author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {58443--58469},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2983149},
  url = {https://ieeexplore.ieee.org/document/9046805/?arnumber=9046805},
  urldate = {2025-03-10},
  abstract = {Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.},
  eventtitle = {{{IEEE Access}}},
  keywords = {_r/2_worth_reading,_st/New,_st/Skimmed,Accidents,automation,Automation,Autonomous vehicles,control,intelligent transportation systems,intelligent vehicles,Planning,Robot sensing systems,robotics,Systems architecture,Task analysis,Vehicle dynamics},
  file = {/Users/maksimploter/Zotero/storage/6GL9TIG3/Yurtsever et al. - 2020 - A Survey of Autonomous Driving Common Practices a.pdf;/Users/maksimploter/Zotero/storage/LTSQ7QBW/9046805.html;/Users/maksimploter/Zotero/storage/R6V6KTS8/9046805.html;/Users/maksimploter/Zotero/storage/WAVQYPIK/9046805.html}
}


@article{matosSurveySensorFailures2024,
  title = {A {{Survey}} on {{Sensor Failures}} in {{Autonomous Vehicles}}: {{Challenges}} and {{Solutions}}},
  shorttitle = {A {{Survey}} on {{Sensor Failures}} in {{Autonomous Vehicles}}},
  author = {Matos, Francisco and Bernardino, Jorge and {João Durães} and Cunha, João},
  date = {2024-01},
  journaltitle = {Sensors},
  volume = {24},
  number = {16},
  pages = {5108},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s24165108},
  url = {https://www.mdpi.com/1424-8220/24/16/5108},
  urldate = {2025-04-02},
  abstract = {Autonomous vehicles (AVs) rely heavily on sensors to perceive their surrounding environment and then make decisions and act on them. However, these sensors have weaknesses, and are prone to failure, resulting in decision errors by vehicle controllers that pose significant challenges to their safe operation. To mitigate sensor failures, it is necessary to understand how they occur and how they affect the vehicle’s behavior so that fault-tolerant and fault-masking strategies can be applied. This survey covers 108 publications and presents an overview of the sensors used in AVs today, categorizes the sensor’s failures that can occur, such as radar interferences, ambiguities detection, or camera image failures, and provides an overview of mitigation strategies such as sensor fusion, redundancy, and sensor calibration. It also provides insights into research areas critical to improving safety in the autonomous vehicle industry, so that new or more in-depth research may emerge.},
  issue = {16},
  langid = {english},
  keywords = {_st/Skimmed,autonomous vehicles,fault tolerance,redundancy in AVs,sensor failures},
  file = {/Users/maksimploter/Zotero/storage/HN2LZUIK/Matos et al. - 2024 - A Survey on Sensor Failures in Autonomous Vehicles Challenges and Solutions.pdf}
}

@article{parkRealTimeCharacteristicsROS2020,
  title = {Real-{{Time Characteristics}} of {{ROS}} 2.0 in {{Multiagent Robot Systems}}: {{An Empirical Study}}},
  shorttitle = {Real-{{Time Characteristics}} of {{ROS}} 2.0 in {{Multiagent Robot Systems}}},
  author = {Park, Jaeho and Delgado, Raimarius and Choi, Byoung Wook},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {154637--154651},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3018122},
  url = {https://ieeexplore.ieee.org/document/9172073},
  urldate = {2025-05-05},
  abstract = {Due to its ease of use and flexibility, the robot operating system (ROS) is increasingly becoming the most popular middleware for robot applications, even in multiagent systems. Since ROS 1.0 does not satisfy real-time requirements, ROS 2.0 was developed, and it improved the communication stack with the real-time data distribution service (DDS) protocol. However, the actual performance level to be expected is still unknown and can largely depend on the operating system and the kernel being used, the DDS distribution, and the overall software load of the system. In this article, we present an empirical study that evaluates the real-time performance of ROS 2.0 in both the system and communication software layers. In the system layer, the deterministic behavior of the ROS 2.0 nodes is thoroughly observed with regard to whether the tasks are schedulable and can function within the specified temporal deadline. In the communication layer, special attention is devoted to the rate of data loss and the overall latency of messages between nodes. Experiments are performed in various working conditions; for example, the system load is increased to define the real-time performance of the tasks. For reference, the results are compared with the those from the traditional ROS variation. Moreover, we implement a multiagent service robot system to verify the suitability of ROS 2.0 for real-world applications. Our results show that the application of ROS 2.0 is more suitable than that of ROS 1.0 in terms of real-time performance.},
  keywords = {_st/Cited,Multi-agent system,Navigation,performance evaluation,Performance evaluation,real-time operating systems,Real-time systems,Robot kinematics,robot operating system,Software,Task analysis},
  file = {/Users/maksimploter/Zotero/storage/TNWM4BNA/Park et al. - 2020 - Real-Time Characteristics of ROS 2.0 in Multiagent Robot Systems An Empirical Study.pdf}
}

@article{tampuuSurveyEndtoEndDriving2022,
  title = {A {{Survey}} of {{End-to-End Driving}}: {{Architectures}} and {{Training Methods}}},
  shorttitle = {A {{Survey}} of {{End-to-End Driving}}},
  author = {Tampuu, Ardi and Matiisen, Tambet and Semikin, Maksym and Fishman, Dmytro and Muhammad, Naveed},
  date = {2022-04},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {4},
  pages = {1364--1384},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3043505},
  url = {https://ieeexplore.ieee.org/document/9310544/},
  urldate = {2025-05-13},
  abstract = {Autonomous driving is of great interest to industry and academia alike. The use of machine learning approaches for autonomous driving has long been studied, but mostly in the context of perception. In this article, we take a deeper look on the so-called end-to-end approaches for autonomous driving, where the entire driving pipeline is replaced with a single neural network. We review the learning methods, input and output modalities, network architectures, and evaluation schemes in end-to-end driving literature. Interpretability and safety are discussed separately, as they remain challenging for this approach. Beyond providing a comprehensive overview of existing methods, we conclude the review with an architecture that combines the most promising elements of the end-to-end autonomous driving systems.},
  keywords = {_st/Cited,_st/New,Automobiles,Autonomous driving,Autonomous vehicles,end-to-end,neural networks,Neural networks,Pipelines,Roads,Task analysis,Training},
  file = {/Users/maksimploter/Zotero/storage/293B796C/Tampuu et al. - 2022 - A Survey of End-to-End Driving Architectures and Training Methods.pdf}
}


@article{fischlerRepresentationMatchingPictorial1973,
  title = {The {{Representation}} and {{Matching}} of {{Pictorial Structures}}},
  author = {Fischler, M.A. and Elschlager, R.A.},
  date = {1973-01},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-22},
  number = {1},
  pages = {67--92},
  issn = {1557-9956},
  doi = {10.1109/T-C.1973.223602},
  url = {https://ieeexplore.ieee.org/document/1672195/?arnumber=1672195},
  urldate = {2025-04-03},
  abstract = {The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of "goodness" of matching or detection.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {_st/New,Dynamic programming heuristic optimization picture description picture matching picture processing representation.},
  file = {/Users/maksimploter/Zotero/storage/7D3D26TZ/Fischler and Elschlager - 1973 - The Representation and Matching of Pictorial Structures.pdf;/Users/maksimploter/Zotero/storage/ALZK3YD8/1672195.html}
}

@article{hintonReducingDimensionalityData2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  date = {2006-07-28},
  journaltitle = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1127647},
  url = {https://www.science.org/doi/10.1126/science.1127647},
  urldate = {2025-04-03},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  keywords = {_st/Cited,_st/New,:todo:skim_first},
  file = {/Users/maksimploter/Zotero/storage/BIUQRDPQ/Hinton and Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2024-06-28},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  langid = {english},
  keywords = {_st/Cited,_st/New,:todo:skim_first,Computer science,Mathematics and computing},
  file = {/Users/maksimploter/Zotero/storage/G99AILUJ/LeCun et al. - 2015 - Deep learning.pdf}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2025-04-03},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  keywords = {_st/New},
  file = {/Users/maksimploter/Zotero/storage/CCM2P8Y6/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}


@inproceedings{girshickRichFeatureHierarchies2014a,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-06},
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  url = {https://ieeexplore.ieee.org/document/6909475},
  urldate = {2025-04-03},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {_st/New,Feature extraction,Object detection,Proposals,Support vector machines,Training,Vectors,Visualization},
  file = {/Users/maksimploter/Zotero/storage/7C92E6PA/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/maksimploter/Zotero/storage/9SRAIGQR/6909475.html}
}

@online{russakovskyImageNetLargeScale2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2015-01-30},
  eprint = {1409.0575},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.0575},
  url = {http://arxiv.org/abs/1409.0575},
  urldate = {2025-05-05},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/MYSBNVJG/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf;/Users/maksimploter/Zotero/storage/T998WMCG/1409.html}
}


@article{jiaoNewGenerationDeep2022,
  title = {New {{Generation Deep Learning}} for {{Video Object Detection}}: {{A Survey}}},
  shorttitle = {New {{Generation Deep Learning}} for {{Video Object Detection}}},
  author = {Jiao, Licheng and Zhang, Ruohan and Liu, Fang and Yang, Shuyuan and Hou, Biao and Li, Lingling and Tang, Xu},
  date = {2022-08},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {8},
  pages = {3195--3215},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3053249},
  url = {https://ieeexplore.ieee.org/document/9345705/?arnumber=9345705},
  urldate = {2024-09-04},
  abstract = {Video object detection, a basic task in the computer vision field, is rapidly evolving and widely used. In recent years, deep learning methods have rapidly become widespread in the field of video object detection, achieving excellent results compared with those of traditional methods. However, the presence of duplicate information and abundant spatiotemporal information in video data poses a serious challenge to video object detection. Therefore, in recent years, many scholars have investigated deep learning detection algorithms in the context of video data and have achieved remarkable results. Considering the wide range of applications, a comprehensive review of the research related to video object detection is both a necessary and challenging task. This survey attempts to link and systematize the latest cutting-edge research on video object detection with the goal of classifying and analyzing video detection algorithms based on specific representative models. The differences and connections between video object detection and similar tasks are systematically demonstrated, and the evaluation metrics and video detection performance of nearly 40 models on two data sets are presented. Finally, the various applications and challenges facing video object detection are discussed.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Convolution,Deep learning,Detection algorithms,Feature extraction,Learning,Learning systems,neural networks,object detection,Object detection,pipeline processing,Survey,Task analysis,video signal processing},
  file = {/Users/maksimploter/Zotero/storage/PZN6AJZ2/Jiao et al. - 2022 - New Generation Deep Learning for Video Object Dete.pdf;/Users/maksimploter/Zotero/storage/TBVS8TBF/Jiao et al. - 2022 - New Generation Deep Learning for Video Object Detection A Survey.pdf;/Users/maksimploter/Zotero/storage/73S44XGH/9345705.html}
}

@online{hanSeqNMSVideoObject2016,
  title = {Seq-{{NMS}} for {{Video Object Detection}}},
  author = {Han, Wei and Khorrami, Pooya and Paine, Tom Le and Ramachandran, Prajit and Babaeizadeh, Mohammad and Shi, Honghui and Li, Jianan and Yan, Shuicheng and Huang, Thomas S.},
  date = {2016-08-22},
  eprint = {1602.08465},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.08465},
  url = {http://arxiv.org/abs/1602.08465},
  urldate = {2025-03-27},
  abstract = {Video object detection is challenging because objects that are easily detected in one frame may be difficult to detect in another frame within the same clip. Recently, there have been major advances for doing object detection in a single image. These methods typically contain three phases: (i) object proposal generation (ii) object classification and (iii) post-processing. We propose a modification of the post-processing phase that uses high-scoring object detections from nearby frames to boost scores of weaker detections within the same clip. We show that our method obtains superior results to state-of-the-art single image object detection techniques. Our method placed 3rd in the video object detection (VID) task of the ImageNet Large Scale Visual Recognition Challenge 2015 (ILSVRC2015).},
  pubstate = {prepublished},
  keywords = {_st/Cited,Computer Science - Computer Vision and Pattern Recognition,refining detections,video object detection},
  file = {/Users/maksimploter/Zotero/storage/BHMBB4WF/Han et al. - 2016 - Seq-NMS for Video Object Detection.pdf;/Users/maksimploter/Zotero/storage/8HVHZJ8A/1602.html}
}

@article{kangTCNNTubeletsConvolutional2018,
  title = {T-{{CNN}}: {{Tubelets With Convolutional Neural Networks}} for {{Object Detection From Videos}}},
  shorttitle = {T-{{CNN}}},
  author = {Kang, Kai and Li, Hongsheng and Yan, Junjie and Zeng, Xingyu and Yang, Bin and Xiao, Tong and Zhang, Cong and Wang, Zhe and Wang, Ruohui and Wang, Xiaogang and Ouyang, Wanli},
  date = {2018-10},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {10},
  pages = {2896--2907},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2017.2736553},
  url = {https://ieeexplore.ieee.org/document/8003302},
  urldate = {2025-03-27},
  abstract = {The state-of-the-art performance for object detection has been significantly improved over the past two years. Besides the introduction of powerful deep neural networks, such as GoogleNet and VGG, novel object detection frameworks, such as R-CNN and its successors, Fast R-CNN, and Faster R-CNN, play an essential role in improving the state of the art. Despite their effectiveness on still images, those frameworks are not specifically designed for object detection from videos. Temporal and contextual information of videos are not fully investigated and utilized. In this paper, we propose a deep learning framework that incorporates temporal and contextual information from tubelets obtained in videos, which dramatically improves the baseline performance of existing still-image detection frameworks when they are applied to videos. It is called T-CNN, i.e., tubelets with convolutional neueral networks. The proposed framework won newly introduced an object-detection-from-video task with provided data in the ImageNet Large-Scale Visual Recognition Challenge 2015. Code is publicly available at https://github.com/myfavouritekk/T-CNN.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}} for {{Video Technology}}},
  keywords = {_st/Cited,_st/Skimmed,computer vision,Convolutional codes,neural networks,Neural networks,Object detection,Proposals,Target tracking,Training,Videos},
  file = {/Users/maksimploter/Zotero/storage/CHRGRNR9/Kang et al. - 2018 - T-CNN Tubelets With Convolutional Neural Networks for Object Detection From Videos.pdf;/Users/maksimploter/Zotero/storage/B8ZQA63K/8003302.html}
}

@inproceedings{kangObjectDetectionVideo2016,
  title = {Object {{Detection}} from {{Video Tubelets}} with {{Convolutional Neural Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kang, Kai and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
  date = {2016-06},
  eprint = {1604.04053},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {817--825},
  doi = {10.1109/CVPR.2016.95},
  url = {http://arxiv.org/abs/1604.04053},
  urldate = {2025-03-27},
  abstract = {Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (R-CNN)). The lately introduced ImageNet task on object detection from video (VID) brings the object detection task into the video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task.},
  keywords = {_r/3_worth_citing,_r/4_should_cite,_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition,video object detection},
  file = {/Users/maksimploter/Zotero/storage/67B2K9SH/Kang et al. - 2016 - Object Detection from Video Tubelets with Convolutional Neural Networks.pdf;/Users/maksimploter/Zotero/storage/LKSEAZWN/1604.html}
}

@InProceedings{Lu_2017_ICCV,
author = {Lu, Yongyi and Lu, Cewu and Tang, Chi-Keung},
title = { Online Video Object Detection Using Association LSTM},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@article{liuSSDSingleShot2016,
	title = {SSD: Single Shot MultiBox Detector},
	doi = {10.1007/978-3-319-46448-0_2},
	author = {Liu, W. and Anguelov, Dragomir and Erhan, D. and Szegedy, Christian and Reed, Scott E. and Fu, Cheng-Yang and Berg, A.},
	journal = {European Conference on Computer Vision},
	year = {2015},
	litmapsId = {276052656}
}

@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation},
  title={Long Short-Term Memory},
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}
}

@misc{xiaoVideoObjectDetection2018,
  title = {Video {{Object Detection}} with an {{Aligned Spatial-Temporal Memory}}},
  author = {Xiao, Fanyi and Lee, Yong Jae},
  year = {2018},
  month = jul,
  number = {arXiv:1712.06317},
  eprint = {1712.06317},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.06317},
  urldate = {2025-04-07},
  abstract = {We introduce Spatial-Temporal Memory Networks for video object detection. At its core, a novel Spatial-Temporal Memory module (STMM) serves as the recurrent computation unit to model long-term temporal appearance and motion dynamics. The STMM's design enables full integration of pretrained backbone CNN weights, which we find to be critical for accurate detection. Furthermore, in order to tackle object motion in videos, we propose a novel MatchTrans module to align the spatial-temporal memory from frame to frame. Our method produces state-of-the-art results on the benchmark ImageNet VID dataset, and our ablative studies clearly demonstrate the contribution of our different design choices. We release our code and models at http://fanyix.cs.ucdavis.edu/project/stmn/project.html.},
  archiveprefix = {arXiv},
  keywords = {_st/Skimmed,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/KK76QNMC/Xiao and Lee - 2018 - Video Object Detection with an Aligned Spatial-Temporal Memory.pdf;/Users/maksimploter/Zotero/storage/LRYHDLY5/1712.html}
}


@online{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder-Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=false and Bahdanau, Dzmitry and Bengio, Yoshua},
  date = {2014-10-07},
  eprint = {1409.1259},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.1259},
  url = {http://arxiv.org/abs/1409.1259},
  urldate = {2025-05-08},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/maksimploter/Zotero/storage/55D4YK45/Cho et al. - 2014 - On the Properties of Neural Machine Translation Encoder-Decoder Approaches.pdf;/Users/maksimploter/Zotero/storage/MP46AMVV/1409.html}
}

@misc{ballasDelvingDeeperConvolutional2016,
  title = {Delving {{Deeper}} into {{Convolutional Networks}} for {{Learning Video Representations}}},
  author = {Ballas, Nicolas and Yao, Li and Pal, Chris and Courville, Aaron},
  year = {2016},
  month = mar,
  number = {arXiv:1511.06432},
  eprint = {1511.06432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06432},
  urldate = {2025-04-08},
  abstract = {We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call "percepts" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.},
  archiveprefix = {arXiv},
  keywords = {_r/4_should_cite,_st/New,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/maksimploter/Zotero/storage/8P8DALFN/Ballas et al. - 2016 - Delving Deeper into Convolutional Networks for Learning Video Representations.pdf;/Users/maksimploter/Zotero/storage/W3L9ZQBZ/1511.html}
}

@online{liuMobileVideoObject2018,
  title = {Mobile {{Video Object Detection}} with {{Temporally-Aware Feature Maps}}},
  author = {Liu, Mason and Zhu, Menglong},
  date = {2018-03-28},
  eprint = {1711.06368},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.06368},
  url = {http://arxiv.org/abs/1711.06368},
  urldate = {2025-04-07},
  abstract = {This paper introduces an online model for object detection in videos designed to run in real-time on low-powered mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/Skimmed,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/EERM3MMP/Liu and Zhu - 2018 - Mobile Video Object Detection with Temporally-Aware Feature Maps.pdf;/Users/maksimploter/Zotero/storage/LHCBF7WA/1711.html}
}

@online{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  date = {2017-04-17},
  eprint = {1704.04861},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.04861},
  url = {http://arxiv.org/abs/1704.04861},
  urldate = {2025-05-15},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  pubstate = {prepublished},
  keywords = {_st/Cited,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/FRZZBCT5/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications.pdf;/Users/maksimploter/Zotero/storage/DUUVL2GV/1704.html}
}

@article{ittiModelSaliencybasedVisual1998,
  title = {A Model of Saliency-Based Visual Attention for Rapid Scene Analysis},
  author = {Itti, L. and Koch, C. and Niebur, E.},
  date = {1998-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {11},
  pages = {1254--1259},
  issn = {1939-3539},
  doi = {10.1109/34.730558},
  url = {https://ieeexplore.ieee.org/document/730558},
  urldate = {2025-05-14},
  abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.},
  keywords = {_st/Cited,Biological system modeling,Brain modeling,Computer architecture,Feature extraction,Hardware,Image analysis,Layout,Neural networks,Object detection,Visual system},
  file = {/Users/maksimploter/Zotero/storage/GTUZ2MAC/Itti et al. - 1998 - A model of saliency-based visual attention for rapid scene analysis.pdf}
}

@online{shiConvolutionalLSTMNetwork2015,
  title = {Convolutional {{LSTM Network}}: {{A Machine Learning Approach}} for {{Precipitation Nowcasting}}},
  shorttitle = {Convolutional {{LSTM Network}}},
  author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  date = {2015-09-19},
  eprint = {1506.04214},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.04214},
  url = {http://arxiv.org/abs/1506.04214},
  urldate = {2024-12-28},
  abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/VDZ2X2P3/Shi et al. - 2015 - Convolutional LSTM Network A Machine Learning App.pdf;/Users/maksimploter/Zotero/storage/RXTQMGFU/1506.html}
}

@online{guoProgressiveSparseLocal2019,
  title = {Progressive {{Sparse Local Attention}} for {{Video}} Object Detection},
  author = {Guo, Chaoxu and Fan, Bin and Gu, Jie and Zhang, Qian and Xiang, Shiming and Prinet, Veronique and Pan, Chunhong},
  date = {2019-08-16},
  eprint = {1903.09126},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1903.09126},
  url = {http://arxiv.org/abs/1903.09126},
  urldate = {2025-05-12},
  abstract = {Transferring image-based object detectors to the domain of videos remains a challenging problem. Previous efforts mostly exploit optical flow to propagate features across frames, aiming to achieve a good trade-off between accuracy and efficiency. However, introducing an extra model to estimate optical flow can significantly increase the overall model size. The gap between optical flow and high-level features can also hinder it from establishing spatial correspondence accurately. Instead of relying on optical flow, this paper proposes a novel module called Progressive Sparse Local Attention (PSLA), which establishes the spatial correspondence between features across frames in a local region with progressively sparser stride and uses the correspondence to propagate features. Based on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are proposed to model temporal appearance and enrich feature representation respectively in a novel video object detection framework. Experiments on ImageNet VID show that our method achieves the best accuracy compared to existing methods with smaller model size and acceptable runtime speed.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/YXRH6PRC/Guo et al. - 2019 - Progressive Sparse Local Attention for Video object detection.pdf;/Users/maksimploter/Zotero/storage/89E6LK76/1903.html}
}

@inproceedings{daiDeformableConvolutionalNetworks2017a,
  title = {Deformable {{Convolutional Networks}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  date = {2017-10},
  pages = {764--773},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.89},
  url = {https://ieeexplore.ieee.org/document/8237351},
  urldate = {2025-05-14},
  abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {_st/CIted,Convolution,Feature extraction,Kernel,Object detection,Standards,Two dimensional displays},
  file = {/Users/maksimploter/Zotero/storage/V66VBB4W/Dai et al. - 2017 - Deformable Convolutional Networks.pdf}
}

@online{bahdanauNeuralMachineTranslation2016a,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  eprint = {1409.0473},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2025-05-12},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/maksimploter/Zotero/storage/MZXBRFL6/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf;/Users/maksimploter/Zotero/storage/JYTPHI2L/1409.html}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-06-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {_r/5_classical_cite,_st/Skimmed,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/maksimploter/Zotero/storage/T8DF74YF/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@online{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  date = {2020-05-28},
  eprint = {2005.12872},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.12872},
  urldate = {2024-07-04},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/WEDNLTW2/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf}
}

@online{zhuDeformableDETRDeformable2021,
  title = {Deformable {{DETR}}: {{Deformable Transformers}} for {{End-to-End Object Detection}}},
  shorttitle = {Deformable {{DETR}}},
  author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  date = {2021-03-17},
  eprint = {2010.04159},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.04159},
  urldate = {2024-07-05},
  abstract = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https:// github.com/fundamentalvision/Deformable-DETR.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/5W4CL25Z/Zhu et al. - 2021 - Deformable DETR Deformable Transformers for End-t.pdf}
}

@online{linD^2ETRDecoderOnlyDETR2022,
  title = {D\textasciicircum{{2ETR}}: {{Decoder-Only DETR}} with {{Computationally Efficient Cross-Scale Attention}}},
  shorttitle = {D\textasciicircum{{2ETR}}},
  author = {Lin, Junyu and Mao, Xiaofeng and Chen, Yuefeng and Xu, Lei and He, Yuan and Xue, Hui},
  date = {2022-03-02},
  eprint = {2203.00860},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.00860},
  url = {http://arxiv.org/abs/2203.00860},
  urldate = {2025-05-14},
  abstract = {DETR is the first fully end-to-end detector that predicts a final set of predictions without post-processing. However, it suffers from problems such as low performance and slow convergence. A series of works aim to tackle these issues in different ways, but the computational cost is yet expensive due to the sophisticated encoder-decoder architecture. To alleviate this issue, we propose a decoder-only detector called D\textasciicircum 2ETR. In the absence of encoder, the decoder directly attends to the fine-fused feature maps generated by the Transformer backbone with a novel computationally efficient cross-scale attention module. D\textasciicircum 2ETR demonstrates low computational complexity and high detection accuracy in evaluations on the COCO benchmark, outperforming DETR and its variants.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/P439DYYZ/Lin et al. - 2022 - D^2ETR Decoder-Only DETR with Computationally Efficient Cross-Scale Attention.pdf;/Users/maksimploter/Zotero/storage/U9WBQDTL/2203.html}
}

@article{zhouTransVODEndtoEndVideo2023,
  title = {{{TransVOD}}: {{End-to-End Video Object Detection}} with {{Spatial-Temporal Transformers}}},
  shorttitle = {{{TransVOD}}},
  author = {Zhou, Qianyu and Li, Xiangtai and He, Lu and Yang, Yibo and Cheng, Guangliang and Tong, Yunhai and Ma, Lizhuang and Tao, Dacheng},
  date = {2023-06-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {45},
  number = {6},
  eprint = {2201.05047},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {7853--7869},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3223955},
  url = {http://arxiv.org/abs/2201.05047},
  urldate = {2024-10-04},
  abstract = {Detection Transformer (DETR) and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, the first end-to-end video object detection system based on simple yet effective spatial-temporal Transformer architectures. The first goal of this paper is to streamline the pipeline of current VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow model, relation networks. Besides, benefited from the object query design in DETR, our method does not need post-processing methods such as Seq-NMS. In particular, we present a temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal transformer consists of two components: Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3 \%-4 \% mAP) on the ImageNet VID dataset. TransVOD yields comparable performances on the benchmark of ImageNet VID. Then, we present two improved versions of TransVOD including TransVOD++ and TransVOD Lite. The former fuses object-level information into object query via dynamic convolution while the latter models the entire video clips as the output to speed up the inference time. We give detailed analysis of all three models in the experiment part. In particular, our proposed TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet VID with 90.0 \% mAP. Our proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 \% mAP while running at around 30 FPS on a single V100 GPU device. Code and models are available at https://github.com/SJTU-LuHe/TransVOD.},
  langid = {english},
  keywords = {_st/New,Computer Science - Computer Vision and Pattern Recognition,Detectors,Fuses,Object detection,Pipelines,scene understanding,Streaming media,Task analysis,transformer,Transformers,video object detection,Video object detection,video understanding,vision transformers},
  file = {/Users/maksimploter/Zotero/storage/58DR6FWM/Zhou et al. - 2023 - TransVOD End-to-End Video Object Detection with S.pdf;/Users/maksimploter/Zotero/storage/9BNNF8Y7/Zhou et al. - 2023 - TransVOD End-to-End Video Object Detection With S.pdf;/Users/maksimploter/Zotero/storage/E386HWFB/Zhou et al. - 2023 - TransVOD End-to-End Video Object Detection With S.pdf;/Users/maksimploter/Zotero/storage/2BQWWFYR/9960850.html;/Users/maksimploter/Zotero/storage/NZ496EDS/9960850.html;/Users/maksimploter/Zotero/storage/Y2GU6A6Q/2201.html}
}

@online{wangPTSEFormerProgressiveTemporalSpatial2022,
  title = {{{PTSEFormer}}: {{Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection}}},
  shorttitle = {{{PTSEFormer}}},
  author = {Wang, Han and Tang, Jun and Liu, Xiaodong and Guan, Shanyan and Xie, Rong and Song, Li},
  date = {2022-09-06},
  eprint = {2209.02242},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.02242},
  url = {http://arxiv.org/abs/2209.02242},
  urldate = {2025-05-14},
  abstract = {Recent years have witnessed a trend of applying context frames to boost the performance of object detection as video object detection. Existing methods usually aggregate features at one stroke to enhance the feature. These methods, however, usually lack spatial information from neighboring frames and suffer from insufficient feature aggregation. To address the issues, we perform a progressive way to introduce both temporal information and spatial information for an integrated enhancement. The temporal information is introduced by the temporal feature aggregation model (TFAM), by conducting an attention mechanism between the context frames and the target frame (i.e., the frame to be detected). Meanwhile, we employ a Spatial Transition Awareness Model (STAM) to convey the location transition information between each context frame and target frame. Built upon a transformer-based detector DETR, our PTSEFormer also follows an end-to-end fashion to avoid heavy post-processing procedures while achieving 88.1\% mAP on the ImageNet VID dataset. Codes are available at https://github.com/Hon-Wong/PTSEFormer.},
  pubstate = {prepublished},
  keywords = {_st/Cited,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/DWPFNF6G/Wang et al. - 2022 - PTSEFormer Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection.pdf;/Users/maksimploter/Zotero/storage/PF3UFA2L/2209.html}
}

@online{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  date = {2021-06-22},
  eprint = {2103.03206},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.03206},
  urldate = {2024-06-28},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domainspecific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver – a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/maksimploter/Zotero/storage/7SFRYG4J/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf}
}

%~~~~~~~~~~~~~~~~~~~METHOD~~~~~~~~~~~~~~~~~~~

@misc{srivastava2016unsupervisedlearningvideorepresentations,
      title={Unsupervised Learning of Video Representations using LSTMs}, 
      author={Nitish Srivastava and Elman Mansimov and Ruslan Salakhutdinov},
      year={2016},
      eprint={1502.04681},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.04681}, 
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  url = {https://ieeexplore.ieee.org/document/726791},
  urldate = {2025-05-14},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {_st/Cited,Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/maksimploter/Zotero/storage/QUDEJZDR/Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf}
}


@article{everinghamPascalVisualObject2010,
  title = {The {{Pascal Visual Object Classes}} ({{VOC}}) {{Challenge}}},
  author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  date = {2010-06-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {88},
  number = {2},
  pages = {303--338},
  issn = {1573-1405},
  doi = {10.1007/s11263-009-0275-4},
  url = {https://doi.org/10.1007/s11263-009-0275-4},
  urldate = {2025-04-28},
  abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
  langid = {english},
  keywords = {_st/New,Artificial Intelligence,Benchmark,Database,Object detection,Object recognition},
  file = {/Users/maksimploter/Zotero/storage/GBFJDGPA/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf}
}

@online{linMicrosoftCOCOCommon2015a,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-21},
  eprint = {1405.0312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1405.0312},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2025-04-28},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  pubstate = {prepublished},
  keywords = {_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/8TYCVHII/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/Users/maksimploter/Zotero/storage/3NRGC7K3/1405.html}
}

@online{stewartEndtoendPeopleDetection2015,
  title = {End-to-End People Detection in Crowded Scenes},
  author = {Stewart, Russell and Andriluka, Mykhaylo},
  date = {2015-07-08},
  eprint = {1506.04878},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.04878},
  urldate = {2024-08-07},
  abstract = {Current people detectors operate either by scanning an image in a sliding window fashion or by classifying a discrete set of proposals. We propose a model that is based on decoding an image into a set of people detections. Our system takes an image as input and directly outputs a set of distinct detection hypotheses. Because we generate predictions jointly, common post-processing steps such as non-maximum suppression are unnecessary. We use a recurrent LSTM layer for sequence generation and train our model end-to-end with a new loss function that operates on sets of detections. We demonstrate the effectiveness of our approach on the challenging task of detecting people in crowded scenes.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/7EQFV3ER/Stewart and Andriluka - 2015 - End-to-end people detection in crowded scenes.pdf}
}

@online{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2018-02-07},
  eprint = {1708.02002},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02002},
  url = {http://arxiv.org/abs/1708.02002},
  urldate = {2025-01-16},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  pubstate = {prepublished},
  version = {2},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/W4JY7FFF/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf;/Users/maksimploter/Zotero/storage/2DLSHUN5/1708.html}
}

@online{rezatofighiGeneralizedIntersectionUnion2019,
  title = {Generalized {{Intersection}} over {{Union}}: {{A Metric}} and {{A Loss}} for {{Bounding Box Regression}}},
  shorttitle = {Generalized {{Intersection}} over {{Union}}},
  author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  date = {2019-04-15},
  eprint = {1902.09630},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.09630},
  url = {http://arxiv.org/abs/1902.09630},
  urldate = {2025-05-03},
  abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that \$IoU\$ can be directly used as a regression loss. However, \$IoU\$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of \$IoU\$ by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized \$IoU\$ (\$GIoU\$) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, \$IoU\$ based, and new, \$GIoU\$ based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/maksimploter/Zotero/storage/FSMDVCDN/Rezatofighi et al. - 2019 - Generalized Intersection over Union A Metric and A Loss for Bounding Box Regression.pdf;/Users/maksimploter/Zotero/storage/TSUZ8CH9/1902.html}
}

@online{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-05-09},
  eprint = {1506.02640},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2024-07-31},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/M5YCNGWW/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf}
}

@article{kuhnHungarianMethodAssignment1955,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  date = {1955},
  journaltitle = {Naval Research Logistics Quarterly},
  volume = {2},
  number = {1--2},
  pages = {83--97},
  issn = {1931-9193},
  doi = {10.1002/nav.3800020109},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
  urldate = {2025-05-03},
  abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
  langid = {english},
  keywords = {_st/New _st/Cited},
  file = {/Users/maksimploter/Zotero/storage/LG3XKK4H/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf;/Users/maksimploter/Zotero/storage/94HGMX8K/nav.html}
}

%~~~~~~~~~~~~~~~~~~~EXPERIMENTS~~~~~~~~~~~~~~~~~~~

@online{loshchilovDecoupledWeightDecay2019a,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2025-05-04},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/maksimploter/Zotero/storage/TIDCDZPV/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/maksimploter/Zotero/storage/QV42LIAQ/1711.html}
}

@online{redmonYOLO9000BetterFaster2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  shorttitle = {{{YOLO9000}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2016-12-25},
  eprint = {1612.08242},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.08242},
  url = {http://arxiv.org/abs/1612.08242},
  urldate = {2025-05-04},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/EI8XCJBE/Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf;/Users/maksimploter/Zotero/storage/Q3VCFWRI/1612.html}
}

@software{Jocher_Ultralytics_YOLO_2023,
author = {Jocher, Glenn and Qiu, Jing and Chaurasia, Ayush},
license = {AGPL-3.0},
month = jan,
title = {{Ultralytics YOLO}},
url = {https://github.com/ultralytics/ultralytics},
version = {8.0.0},
year = {2023}
}

%~~~~~~~~~~~~~~~~~~~DISCUSSION~~~~~~~~~~~~~~~~~~~

@online{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  date = {2017-04-19},
  eprint = {1612.03144},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.03144},
  urldate = {2024-07-31},
  abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/55GBI85E/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf}
}