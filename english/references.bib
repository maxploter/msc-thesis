%~~~~~~~~~~~~~~~~~~INTRODUCTION~~~~~~~~~~~~~~~~~~
@article{litmanAutonomousVehicleImplementationb,
  title = {Autonomous {{Vehicle Implementation Predictions}}: {{Implications}} for {{Transport Planning}}},
  author = {Litman, Todd},
  abstract = {This report explores the impacts of autonomous (also called self-driving, driverless or robotic) vehicles, and their implications for transportation planning. It investigates how quickly such vehicles are likely to develop and be deployed based on experience with previous vehicle technologies; their likely benefits and costs; how they will affect travel activity; and their impacts on road, parking and public transit planning. This analysis indicates that Level 5 autonomous vehicles, able to operate without a driver, may be commercially available and legal to use in some jurisdictions by the late 2020s, but will initially have high costs and limited performance. Some benefits, such as independent mobility for affluent non-drivers, may begin in the 2030s but most impacts, including reduced traffic and parking congestion, independent mobility for low-income people (and therefore reduced need for public transit), increased safety, energy conservation and pollution reductions, will only be significant when autonomous vehicles become common and affordable, probably in the 2040s to 2060s, and some benefits may require dedicated autonomous vehicle lanes, which raises social equity concerns.},
  langid = {english},
  keywords = {_st/New},
  file = {/Users/maksimploter/Zotero/storage/3UYQZLD2/Litman - Autonomous Vehicle Implementation Predictions Implications for Transport Planning.pdf}
}



%~~~~~~~~~~~~~~~~~~~BACKGROUND~~~~~~~~~~~~~~~~~~~
@standard{sae:j3016:2021apr,
  organization = {SAE International},
  title        = {Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles},
  number       = {J3016},
  type         = {Surface Vehicle Recommended Practice},
  date         = {2021-04},
  note         = {Revision April 2021, Superseding J3016 JUN2018},
  % Optional: Add version field if preferred over the note
  % version   = {APR2021}, 
  % Optional: Add location if needed by your style
  % location  = {Warrendale, PA},
}

@article{fischlerRepresentationMatchingPictorial1973,
  title = {The {{Representation}} and {{Matching}} of {{Pictorial Structures}}},
  author = {Fischler, M.A. and Elschlager, R.A.},
  date = {1973-01},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-22},
  number = {1},
  pages = {67--92},
  issn = {1557-9956},
  doi = {10.1109/T-C.1973.223602},
  url = {https://ieeexplore.ieee.org/document/1672195/?arnumber=1672195},
  urldate = {2025-04-03},
  abstract = {The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of "goodness" of matching or detection.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {_st/New,Dynamic programming heuristic optimization picture description picture matching picture processing representation.},
  file = {/Users/maksimploter/Zotero/storage/7D3D26TZ/Fischler and Elschlager - 1973 - The Representation and Matching of Pictorial Structures.pdf;/Users/maksimploter/Zotero/storage/ALZK3YD8/1672195.html}
}

@article{hintonReducingDimensionalityData2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  date = {2006-07-28},
  journaltitle = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1127647},
  url = {https://www.science.org/doi/10.1126/science.1127647},
  urldate = {2025-04-03},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  keywords = {_st/Cited,_st/New,:todo:skim_first},
  file = {/Users/maksimploter/Zotero/storage/BIUQRDPQ/Hinton and Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2024-06-28},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  langid = {english},
  keywords = {_st/Cited,_st/New,:todo:skim_first,Computer science,Mathematics and computing},
  file = {/Users/maksimploter/Zotero/storage/G99AILUJ/LeCun et al. - 2015 - Deep learning.pdf}
}

@inproceedings{girshickRichFeatureHierarchies2014a,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-06},
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  url = {https://ieeexplore.ieee.org/document/6909475},
  urldate = {2025-04-03},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {_st/New,Feature extraction,Object detection,Proposals,Support vector machines,Training,Vectors,Visualization},
  file = {/Users/maksimploter/Zotero/storage/7C92E6PA/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/maksimploter/Zotero/storage/9SRAIGQR/6909475.html}
}

@online{hanSeqNMSVideoObject2016,
  title = {Seq-{{NMS}} for {{Video Object Detection}}},
  author = {Han, Wei and Khorrami, Pooya and Paine, Tom Le and Ramachandran, Prajit and Babaeizadeh, Mohammad and Shi, Honghui and Li, Jianan and Yan, Shuicheng and Huang, Thomas S.},
  date = {2016-08-22},
  eprint = {1602.08465},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.08465},
  url = {http://arxiv.org/abs/1602.08465},
  urldate = {2025-03-27},
  abstract = {Video object detection is challenging because objects that are easily detected in one frame may be difficult to detect in another frame within the same clip. Recently, there have been major advances for doing object detection in a single image. These methods typically contain three phases: (i) object proposal generation (ii) object classification and (iii) post-processing. We propose a modification of the post-processing phase that uses high-scoring object detections from nearby frames to boost scores of weaker detections within the same clip. We show that our method obtains superior results to state-of-the-art single image object detection techniques. Our method placed 3rd in the video object detection (VID) task of the ImageNet Large Scale Visual Recognition Challenge 2015 (ILSVRC2015).},
  pubstate = {prepublished},
  keywords = {_st/Cited,Computer Science - Computer Vision and Pattern Recognition,refining detections,video object detection},
  file = {/Users/maksimploter/Zotero/storage/BHMBB4WF/Han et al. - 2016 - Seq-NMS for Video Object Detection.pdf;/Users/maksimploter/Zotero/storage/8HVHZJ8A/1602.html}
}

@article{kangTCNNTubeletsConvolutional2018,
  title = {T-{{CNN}}: {{Tubelets With Convolutional Neural Networks}} for {{Object Detection From Videos}}},
  shorttitle = {T-{{CNN}}},
  author = {Kang, Kai and Li, Hongsheng and Yan, Junjie and Zeng, Xingyu and Yang, Bin and Xiao, Tong and Zhang, Cong and Wang, Zhe and Wang, Ruohui and Wang, Xiaogang and Ouyang, Wanli},
  date = {2018-10},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {10},
  pages = {2896--2907},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2017.2736553},
  url = {https://ieeexplore.ieee.org/document/8003302},
  urldate = {2025-03-27},
  abstract = {The state-of-the-art performance for object detection has been significantly improved over the past two years. Besides the introduction of powerful deep neural networks, such as GoogleNet and VGG, novel object detection frameworks, such as R-CNN and its successors, Fast R-CNN, and Faster R-CNN, play an essential role in improving the state of the art. Despite their effectiveness on still images, those frameworks are not specifically designed for object detection from videos. Temporal and contextual information of videos are not fully investigated and utilized. In this paper, we propose a deep learning framework that incorporates temporal and contextual information from tubelets obtained in videos, which dramatically improves the baseline performance of existing still-image detection frameworks when they are applied to videos. It is called T-CNN, i.e., tubelets with convolutional neueral networks. The proposed framework won newly introduced an object-detection-from-video task with provided data in the ImageNet Large-Scale Visual Recognition Challenge 2015. Code is publicly available at https://github.com/myfavouritekk/T-CNN.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}} for {{Video Technology}}},
  keywords = {_st/Cited,_st/Skimmed,computer vision,Convolutional codes,neural networks,Neural networks,Object detection,Proposals,Target tracking,Training,Videos},
  file = {/Users/maksimploter/Zotero/storage/CHRGRNR9/Kang et al. - 2018 - T-CNN Tubelets With Convolutional Neural Networks for Object Detection From Videos.pdf;/Users/maksimploter/Zotero/storage/B8ZQA63K/8003302.html}
}

@inproceedings{kangObjectDetectionVideo2016,
  title = {Object {{Detection}} from {{Video Tubelets}} with {{Convolutional Neural Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kang, Kai and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
  date = {2016-06},
  eprint = {1604.04053},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {817--825},
  doi = {10.1109/CVPR.2016.95},
  url = {http://arxiv.org/abs/1604.04053},
  urldate = {2025-03-27},
  abstract = {Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (R-CNN)). The lately introduced ImageNet task on object detection from video (VID) brings the object detection task into the video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task.},
  keywords = {_r/3_worth_citing,_r/4_should_cite,_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition,video object detection},
  file = {/Users/maksimploter/Zotero/storage/67B2K9SH/Kang et al. - 2016 - Object Detection from Video Tubelets with Convolutional Neural Networks.pdf;/Users/maksimploter/Zotero/storage/LKSEAZWN/1604.html}
}
@InProceedings{Lu_2017_ICCV,
author = {Lu, Yongyi and Lu, Cewu and Tang, Chi-Keung},
title = { Online Video Object Detection Using Association LSTM},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}
@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation},
  title={Long Short-Term Memory},
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}
}

@misc{xiaoVideoObjectDetection2018,
  title = {Video {{Object Detection}} with an {{Aligned Spatial-Temporal Memory}}},
  author = {Xiao, Fanyi and Lee, Yong Jae},
  year = {2018},
  month = jul,
  number = {arXiv:1712.06317},
  eprint = {1712.06317},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.06317},
  urldate = {2025-04-07},
  abstract = {We introduce Spatial-Temporal Memory Networks for video object detection. At its core, a novel Spatial-Temporal Memory module (STMM) serves as the recurrent computation unit to model long-term temporal appearance and motion dynamics. The STMM's design enables full integration of pretrained backbone CNN weights, which we find to be critical for accurate detection. Furthermore, in order to tackle object motion in videos, we propose a novel MatchTrans module to align the spatial-temporal memory from frame to frame. Our method produces state-of-the-art results on the benchmark ImageNet VID dataset, and our ablative studies clearly demonstrate the contribution of our different design choices. We release our code and models at http://fanyix.cs.ucdavis.edu/project/stmn/project.html.},
  archiveprefix = {arXiv},
  keywords = {_st/Skimmed,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/KK76QNMC/Xiao and Lee - 2018 - Video Object Detection with an Aligned Spatial-Temporal Memory.pdf;/Users/maksimploter/Zotero/storage/LRYHDLY5/1712.html}
}

@misc{ballasDelvingDeeperConvolutional2016,
  title = {Delving {{Deeper}} into {{Convolutional Networks}} for {{Learning Video Representations}}},
  author = {Ballas, Nicolas and Yao, Li and Pal, Chris and Courville, Aaron},
  year = {2016},
  month = mar,
  number = {arXiv:1511.06432},
  eprint = {1511.06432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06432},
  urldate = {2025-04-08},
  abstract = {We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call "percepts" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.},
  archiveprefix = {arXiv},
  keywords = {_r/4_should_cite,_st/New,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/maksimploter/Zotero/storage/8P8DALFN/Ballas et al. - 2016 - Delving Deeper into Convolutional Networks for Learning Video Representations.pdf;/Users/maksimploter/Zotero/storage/W3L9ZQBZ/1511.html}
}

@online{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  date = {2021-06-22},
  eprint = {2103.03206},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.03206},
  urldate = {2024-06-28},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domainspecific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver – a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/maksimploter/Zotero/storage/7SFRYG4J/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf}
}



%~~~~~~~~~~~~~~~~~~~METHOD~~~~~~~~~~~~~~~~~~~



@misc{srivastava2016unsupervisedlearningvideorepresentations,
      title={Unsupervised Learning of Video Representations using LSTMs}, 
      author={Nitish Srivastava and Elman Mansimov and Ruslan Salakhutdinov},
      year={2016},
      eprint={1502.04681},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.04681}, 
}



%~~~~~~~~~~~~~~~~~~~EXPERIMENTS~~~~~~~~~~~~~~~~~~~


@unpublished{tamara_munzner_keynote_2012,
	location = {Broad Institute},
	title = {Keynote on Visualization Principles},
	url = {https://www.youtube.com/watch?v=ZgOF8R6YL2U},
	abstract = {Copyright Broad Institute, 2013. All rights reserved.
Tamara Munzner (www.bit.ly/tmunzner) presents very lucid and useful guidelines for creating effective visualizations, including how to correctly rank visual channel types and how to use categorical color constraints. She explains advantages of 2D representation and drawbacks of 3D, immersive, or animated visualizations. She also describes how to create visualizations that reduce the viewer's cognitive load, and how to validate visualizations. This talk was presented at {VIZBI} 2011, an international conference series on visualizing biological data (vizbi.org) funded by {NIH} \& {EMBO}. Slides from the talk are at bit.ly/{nCJM}5U.

For information about data visualization efforts at the Broad Institute, please visit:
http://www.broadinstitute.org/node/1363/},
	author = {Tamara Munzner},
	urldate = {2024-12-05},
	date = {2012},
}

@book{graves_strategic_2012,
	edition = {2nd edition},
	title = {A Strategic Guide to Technical Communication},
	isbn = {978-1-55481-107-6},
	abstract = {A Strategic Guide to Technical Communication incorporates useful and specific strategies for writers, to enable them to create aesthetically appealing and usable technical documentation. These strategies have been developed and tested on a thousand students from a number of different disciplines over twelve years and three institutions. The second edition adds a chapter on business communication, reworks the discussion on technical style, and expands the information on visual communication and ethics into free-standing chapters.  The text is accompanied by a passcode-protected website containing materials for instructors ({PowerPoint} lectures, lesson plans, sample student work, and helpful links).},
	pagetotal = {328},
	publisher = {Broadview Press},
	author = {Graves, Heather and Graves, Roger},
	date = {2012-05-23},
}
