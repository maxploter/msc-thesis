\section{Experiments}  \label{Experiments}

This section details the experimental setup and results. It begins by outlining the training procedures for the RPerceiver model in the bounding box prediction task and the RPerceiverMM model for the center point prediction task. Subsequently, a comparative analysis is presented, evaluating the RPerceiver architecture against the YOLOv8n baseline for object detection on the detection-moving-mnist-easy benchmark, with a focus on how spatio-temporal information is utilized and performance in challenging scenarios like overlapping objects and objects near frame borders. Finally, an extensive ablation study investigates the robustness of the RPerceiver and RPerceiverMM models under various conditions, including complete sensor failure and non-deterministic sensor input, across single-view and multi-view configurations using default, shuffle, and blind evaluation procedures.

\subsection{Training for Bounding Boxes Prediction Task} \label{Experiments:TrainingBoundingBoxesTask}

The RPerceiver model was trained for the bounding box prediction task using the AdamW optimizer \cite{loshchilovDecoupledWeightDecay2019a} with an initial learning rate of $10^{-4}$. Training was conducted for 31 epochs, with the learning rate reduced by a factor of 10 at epochs 18 and 28. The training pipeline included data augmentation, normalization based on pre-calculated dataset statistics, and resizing frames to a fixed dimension of $320 \times 320$ pixels \cite{redmonYOLO9000BetterFaster2016}. For a complete list of training hyperparameters, refer to Appendix~\ref{Appendix:TrainingHyperparameters}.

\subsection{Training for Center Points Prediction Task} \label{Experiments:TrainingCenterPointsTask}

The RPerceiverMM model was trained for the center point prediction task using the AdamW optimizer \cite{loshchilovDecoupledWeightDecay2019a} with an initial learning rate of $10^{-4}$. Training was conducted for 21 epochs, with the learning rate reduced by a factor of 10 at epoch 18. The training pipeline included data augmentation and normalization, ensuring robust performance. For a complete list of training hyperparameters, refer to Appendix \ref{Appendix:TrainingHyperparameters}.

\subsection{Comparison Analysis} \label{Experiments:ComparisonAnalysis}

First, the task of object detection using the detection-moving-mnist-easy benchmark from Section \ref{Methods:Dataset} is considered. A comparative analysis against a still-image object detector of comparable size, YOLOv8n \cite{Jocher_Ultralytics_YOLO_2023}, is performed. The primary goal is to evaluate how the proposed RPerceiver architecture utilizes spatio-temporal information from the video input. The results are shown in Table~\ref{tab:model_comparison}.

% YOLO: Raw mAP Results (Evaluator): {'map': tensor(0.9231), 'map_50': tensor(0.9621), 'map_75': tensor(0.9410), 'map_small': tensor(0.9231), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7471), 'mar_10': tensor(0.9393), 'mar_100': tensor(0.9393), 'mar_small': tensor(0.9393), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}

% RPerceiver: Raw mAP Results (Evaluator): {'map': tensor(0.9025), 'map_50': tensor(0.9688), 'map_75': tensor(0.9442), 'map_small': tensor(0.9025), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7367), 'mar_10': tensor(0.9240), 'mar_100': tensor(0.9240), 'mar_small': tensor(0.9240), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}

% TODO: recompute GFLOPS

\begin{table}[htb!]
    \centering
    \caption{Comparison with the baseline still image detector YOLOv8n \cite{Jocher_Ultralytics_YOLO_2023} on the detection-moving-mnist-easy test split. RPerceiver achieves slightly better $mAP_{50}$ and $mAP_{75}$, but shows worse $mAP_{50-95}$ results. However, RPerceiver achieves these results with significantly fewer parameters and lower computational cost.}
    \label{tab:model_comparison}
    \begin{tblr}{width=1\textwidth, hlines, vlines,
                  colspec = { l c c c c c },
                  row{1} = {font=\bfseries},
                 }
        Model      & mAP_{50} & mAP_{75} & mAP_{50-95} & Params (M)   & GFLOPS         \\
        YOLOv8n    & 96.2  & 94.1 & \textbf{92.3}  & 3            & 8.1            \\
        RPerceiver & \textbf{96.9} & \textbf{94.4} & 90.3 &\textbf{1} & \textbf{1.2}   \\
    \end{tblr}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_val_batch2_pred_recurrent_perceiver.jpg}
    \caption{Visualization of RPerceiver predictions across sequential frames. Bounding boxes and confidence scores are displayed for detected digits. Notably, confidence scores can be less than 1.0 even for clear objects (e.g. digits in frame 1). Two challenging scenarios are analyzed: overlapping digits and digits near the frame border. The model shows limitations such as exhibiting false negatives with overlapping digits, as illustrated by the central cluster involving digits such as '1's and a '2' between frames 8 and 15. In contrast, predictions appear consistent between frames 5 and 13 for several digits near frame borders: the '2' and '6' on the left border, the '6' and '8' near the bottom border, and the '6' on the right border.}
    \label{fig:figure_methods_val_batch2_pred_recurrent_perceiver}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_val_batch2_pred_YOLO.jpg}
    \caption{Visualization of YOLOv8n \cite{Jocher_Ultralytics_YOLO_2023} predictions across sequential frames. Bounding boxes and confidence scores are displayed for detected digits. Notably, confidence scores are typically high (often 1.0) for relatively clear objects (e.g., digits in frame 1). Two challenging scenarios are analyzed: overlapping digits and digits near the frame border. The model appears to handle overlapping digits relatively effectively, with a few false negatives in the central cluster involving digits such as '1's and a '2' between frames 8 and 15. In contrast, predictions appear inconsistent between frames 5 and 13 for several digits near frame borders: the '2' on the left border, the '6' and '8' near the bottom border.}
    \label{fig:figure_methods_val_batch2_pred_YOLO}
\end{figure}

To quantify the observations regarding model performance in challenging scenarios, as illustrated in Figures \ref{fig:figure_methods_val_batch2_pred_recurrent_perceiver} and \ref{fig:figure_methods_val_batch2_pred_YOLO}, the models were specifically evaluated on filtered subsets of the ground truth data. These subsets isolate two challenging conditions: overlapping objects (defined as pairs of ground truth objects with an Intersection over Union (IoU) greater than $10\%$) and border objects (defined as ground truth objects whose bounding boxes intersect with the frame border). These subsets represent $10.4\%$ and $11.5\%$ of the entire test set. The resulting performance metrics for these specific cases are presented in Table \ref{tab:model_comparison_detailed}.

% YOLO:
% ### OVERLAP IOU 0.1

% Raw mAP Results (Evaluator): {'map': tensor(0.9231), 'map_50': tensor(0.9621), 'map_75': tensor(0.9410), 'map_small': tensor(0.9231), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7471), 'mar_10': tensor(0.9393), 'mar_100': tensor(0.9393), 'mar_small': tensor(0.9393), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}


% Raw Overlap mAP Results (Evaluator): {'map': tensor(0.7570), 'map_50': tensor(0.8749), 'map_75': tensor(0.8246), 'map_small': tensor(0.7570), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7398), 'mar_10': tensor(0.7959), 'mar_100': tensor(0.7959), 'mar_small': tensor(0.7959), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}

% Raw Boundary mAP Results (Evaluator): {'map': tensor(0.6989), 'map_50': tensor(0.7667), 'map_75': tensor(0.7365), 'map_small': tensor(0.6989), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7119), 'mar_10': tensor(0.7280), 'mar_100': tensor(0.7280), 'mar_small': tensor(0.7280), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}

% num_total_gt_objects: 851740.0000
% num_overlap_gt_objects: 88680.0000
% num_boundary_gt_objects: 97997.0000
% percentage_overlap_gt_objects: 10.4116
% percentage_boundary_gt_objects: 11.5055

% RPerceiver:
% ### OVERLAP IOU 0.1

% Raw mAP Results (Evaluator): {'map': tensor(0.9025), 'map_50': tensor(0.9688), 'map_75': tensor(0.9442), 'map_small': tensor(0.9025), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.7367), 'mar_10': tensor(0.9240), 'mar_100': tensor(0.9240), 'mar_small': tensor(0.9240), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}


% Raw Overlap mAP Results (Evaluator): {'map': tensor(0.6100), 'map_50': tensor(0.8396), 'map_75': tensor(0.6883), 'map_small': tensor(0.6100), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.6138), 'mar_10': tensor(0.6676), 'mar_100': tensor(0.6676), 'mar_small': tensor(0.6676), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}


% Raw Boundary mAP Results (Evaluator): {'map': tensor(0.8470), 'map_50': tensor(0.9552), 'map_75': tensor(0.9101), 'map_small': tensor(0.8470), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(0.8512), 'mar_10': tensor(0.8739), 'mar_100': tensor(0.8739), 'mar_small': tensor(0.8739), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)}

% num_total_gt_objects: 851740.0000
% num_overlap_gt_objects: 88680.0000
% num_boundary_gt_objects: 97997.0000
% percentage_overlap_gt_objects: 10.4116
% percentage_boundary_gt_objects: 11.5055


\begin{table}[htb!]
    \centering
    \caption{Comparative analysis of YOLOv8n \cite{Jocher_Ultralytics_YOLO_2023} and RPerceiver on specific scenarios within the detection-moving-mnist-easy test split: object overlaps ('Overlap') and proximity to image borders ('Border'). Postprocessing ('Post') is applied to both models. The data indicates that the still-image detector YOLOv8n achieves higher accuracy on overlapping objects. In contrast, RPerceiver significantly outperforms YOLOv8n on border cases, supporting the hypothesis that it effectively leverages temporal information from the video sequence.}
    \label{tab:model_comparison_detailed}
    \begin{tblr}{width=1\textwidth, hlines, vlines,
                  colspec = { l c c c c },
                  row{1} = {font=\bfseries},
                 }
        \SetCell[r=2]{l}Model  & \SetCell[c=3]{c}Overlap & & & \SetCell[c=3]{c}Border & & \\
                   & mAP_{50} & mAP_{75}  & mAP_{50-95}       & mAP_{50} & mAP_{75}  & mAP_{50-95}  \\
        YOLOv8n    & \textbf{87.5}  & \textbf{82.4} & \textbf{75.7} & 76.7  & 73.7 & 69.9 \\
        RPerceiver & 84.0 & 68.8 & 61.0  & \textbf{95.5} & \textbf{91.0} & \textbf{84.7}\\
    \end{tblr}
\end{table}

\subsection{Ablation Study} \label{Experiments:AblationStudy}

This ablation study evaluates the robustness of the RPerceiver and RPerceiverMM models against complete sensor failure and non-deterministic sensor input. The evaluation employs the center point prediction task. Two primary configurations were experimented with: \texttt{single-view} and \texttt{multi-view}. First, the \texttt{single-view} configuration utilized a single camera sensor, representing a basic object detection task using video input. Second, the \texttt{multi-view} setting involved processing information from multiple cameras to perform object detection within a unified spatial representation derived from these inputs.

Three distinct evaluation procedures were considered: \texttt{default}, \texttt{shuffle}, and \texttt{blind}. Additionally, a combination of the latter two procedures was evaluated.

\begin{description}
    \item[\texttt{default}] This procedure represents the normal operational regime where all sensors function as expected without any induced faults or input shuffle.

    \item[\texttt{shuffle}] In this procedure, the sensor inputs are randomly permuted within each time step. Consequently, the model receives inputs from the sensors in a random order for that specific time step. Shuffling occurs only for sensor inputs within the same time step. This procedure is applicable only to the \texttt{multi-view} setup.

    \item[\texttt{blind}] This procedure simulates a complete sensor(s) failure scenario where the input from camera sensor(s) becomes unavailable. The \texttt{blind} procedure is implemented by dropping the input stream from the sensor(s) after a midpoint time step, $t_{half}$. Consequently, information from the sensor(s) is unavailable to the model for the second half of the sequence.
\end{description}

It is important to note that the distinct terms "\texttt{dropout}" for the training procedure described in Section \ref{Methods:DropoutAndShuffle} and "\texttt{blind}" for the evaluation procedure were chosen intentionally, even though they represent a similar goal of removing the input data. The \texttt{blind} procedure can be understood as a specific case of the \texttt{dropout} mechanism, where the probability of dropping a sensor's input is set to 1.0, effectively "blinding" the model to that sensor's input.

The analysis begins by comparing the performance of the baseline RPerceiver (RP) model against its counterpart trained using the dropout procedure (RP (d)), as described in Section \ref{Methods:DropoutAndShuffle}. This comparison is conducted under the \texttt{single-view} configuration using the \texttt{default} evaluation procedure. The results of this evaluation are presented in Table~\ref{tab:results_single_view_default}.

\begin{table}[htb!]
    \centering
    \caption{Comparison of RPerceiver (RP) and RPerceiver trained with dropout (RP (d)) under the \texttt{single-view} configuration and \texttt{default} evaluation procedure. The '(d)' denotes training with dropout. Results are presented based on the number of digits in the input sequence. Metrics shown are Average Displacement Error (ADE), calculated over the second half of the sequence, and Final Displacement Error (FDE), calculated for the final frame. The results indicate that the baseline RPerceiver consistently outperforms the RPerceiver trained with dropout, although the difference is not very large, achieving lower error rates for both ADE and FDE across all tested sequence complexities.
    }
    \label{tab:results_single_view_default}
    \begin{tblr}{width=1\textwidth, hlines, vlines,
                    colspec = { l c c c c c c c c c c },
                    row{1} = {font=\bfseries},
                    row{2} = {font=\bfseries},
                }
        \SetCell[r=2]{l} Model & \SetCell[c=2]{c}1 digit & & \SetCell[c=2]{c}2 digits & & \SetCell[c=2]{c}4 digits & & \SetCell[c=2]{c}8 digits & & \SetCell[c=2]{c}10 digits & \\
        & ADE & FDE & ADE & FDE & ADE & FDE & ADE & FDE & ADE & FDE \\
        RP              & \textbf{0.717} & \textbf{0.725} & \textbf{0.794} & \textbf{0.793} & \textbf{0.958} & \textbf{0.934} & \textbf{1.520} & \textbf{1.382} & \textbf{3.273} & \textbf{2.611} \\
        RP (d) & 0.745 & 0.793 & 0.838 & 0.882 & 1.034 & 1.065 & 1.656 & 1.583 & 3.626 & 3.12 \\
    \end{tblr}
\end{table}

Next, the same models, RPerceiver (RP) and RPerceiver trained with dropout (RP (d)), are compared. This comparison is conducted under the \texttt{single-view} configuration using the \texttt{blind} evaluation procedure. The results for this \texttt{blind} test are detailed in Table~\ref{tab:results_frame_dropout_blind}.

\begin{table}[htb!]
    \centering
    \caption{Comparison of RPerceiver (RP) and RPerceiver trained with dropout (RP (d)) under the \texttt{single-view} configuration and \texttt{blind} evaluation procedure. The '(d)' denotes training with dropout. Results are presented based on the number of digits in the input sequence. Metrics shown are Average Displacement Error (ADE), calculated over the second half of the sequence, and Final Displacement Error (FDE), calculated for the final frame. The results under the \texttt{blind} condition demonstrate a significant advantage for the RPerceiver trained with dropout (RP (d)). This model maintains substantially lower error rates compared to the baseline RP, which exhibits a sharp degradation in performance under the \texttt{blind} evaluation procedure.
    }
    \label{tab:results_frame_dropout_blind}
    \begin{tblr}{width=1\textwidth, hlines, vlines,
                    colspec = { l c c c c c c c c c c },
                    row{1} = {font=\bfseries},
                    row{2} = {font=\bfseries},
                    colsep=3pt,
                }
        \SetCell[r=2]{l} Model & \SetCell[c=2]{c}1 digit & & \SetCell[c=2]{c}2 digits & & \SetCell[c=2]{c}4 digits & & \SetCell[c=2]{c}8 digits & & \SetCell[c=2]{c}10 digits & \\
        & ADE & FDE & ADE & FDE & ADE & FDE & ADE & FDE & ADE & FDE \\
        RP              & 42.210 & 34.754 & 36.087 & 28.83014 & 32.146 & 24.397 & 31.32972 & 23.730 & 33.334 & 25.935 \\
        RP (d)             & \textbf{1.541} & \textbf{1.101} & \textbf{1.907} & \textbf{1.364} & \textbf{2.658} & \textbf{1.905} & \textbf{5.084} & \textbf{3.679} & \textbf{8.198} & \textbf{6.627} \\
    \end{tblr}
\end{table}

For the \texttt{multi-view} configuration, a tile augmentation technique is introduced to the dataset pipeline. This process divides each frame into a specified grid of views, partitioning the corresponding ground truths as well. In the experiments, a $2 \times 2$ grid configuration is utilized. This effectively treats the tiled frame as a simplified spatial layout, akin to a Bird's-Eye View (BEV) where each tile represents a spatial region. Figure~\ref{fig:figure_methods_dataset_image_view_bev} illustrates an example of a frame divided using this grid approach.

\begin{figure}
    \centering
    \includegraphics[width=0.33\textwidth]{figures/figure_methods_dataset_image_view_bev.png}
    \caption{Bird Eye View (BEV) of the frame raster divided into grid.}
    \label{fig:figure_methods_dataset_image_view_bev}
\end{figure}

The ablation study is now extended to the \texttt{multi-view} configuration, evaluating the RPerceiverMM (RPMM) model and its variants trained with training procedures from Section \ref{Methods:DropoutAndShuffle}: shuffle training (RPMM (s)), dropout training (RPMM (d)), and combined shuffle and dropout training (RPMM (d, s)). These models are compared against the baseline RPMM under four distinct evaluation procedures: \texttt{default}, \texttt{shuffle}, \texttt{blind}, and a combined \texttt{blind} and \texttt{shuffle} scenario. The performance metrics for these multi-view evaluations are presented in Table~\ref{tab:results_multi_view}.

% Results: https://docs.google.com/spreadsheets/d/1shITm2iWIKzAAlWpRwED-g1H6YurMJoJgxagRU2x6Gg/edit?gid=290835343#gid=290835343

\begin{table}[htb!]
    \centering
    \caption{Comparison of RPerceiverMM (RPMM) variants under the \texttt{multi-view} configuration across different evaluation procedures: \texttt{default}, \texttt{shuffle}, \texttt{blind}, and combined \texttt{blind, shuffle}. Model notations indicate training procedures: '(s)' for shuffle training, '(d)' for dropout training, and '(d, s)' for combined dropout and shuffle training. Metrics shown are Average Displacement Error (ADE), calculated over the second half of the sequence, Final Displacement Error (FDE), calculated for the final frame, and the number of model parameters in millions (Params (M)). The results highlight trade-offs: baseline RPMM performs best under \texttt{default} conditions. While these training procedures introduce a small performance penalty in the \texttt{default} evaluation, they provide substantial improvements under performance-degrading evaluation strategies. Models trained with shuffle (s) excel in the \texttt{shuffle} evaluation, while dropout-trained models (d) show superior robustness in the \texttt{blind} scenario. The combined training (d, s) yields the most robust performance under the combined \texttt{blind, shuffle} condition, demonstrating the effectiveness of targeted training strategies for specific failure modes. Interestingly, RPMM (d) also shows notable robustness under the \texttt{shuffle} evaluation, despite not being explicitly trained for this condition.
    }
    \label{tab:results_multi_view}
    \begin{tblr}{
        hlines, vlines,
        colspec={l c c c c c c c c c},
        row{1}={font=\bfseries},
        row{2}={font=\bfseries},
    }
        \SetCell[r=2]{l}Model & \SetCell[c=2]{c}Default & & \SetCell[c=2]{c}Shuffle & & \SetCell[c=2]{c}Blind & & \SetCell[c=2]{c}Blind, Shuffle & & \SetCell[r=2]{l}{Params \\ (M)} \\
        & ADE & FDE & ADE & FDE & ADE & FDE & ADE & FDE &\\
        RPMM & 0.760 & 0.753 & 23.701 & 23.759 & 17.952 & 25.568 & 25.559 & 24.696 & 1.2 \\
        RPMM (s) & 0.823 & 0.814 & 0.812 & 0.799 & 13.644 & 20.124 & 13.670 & 20.080 & 1.2 \\
        RPMM (d) & 0.881 & 0.937 & 4.043 & 4.345 & 1.471 & 2.093 & 5.648 & 7.341 & 1.2 \\
        RPMM (d, s) & 1.073 & 1.152 & 0.956 & 1.022 & 1.729 & 2.345 & 1.632 & 2.287 & 1.2 \\
    \end{tblr}
\end{table}
