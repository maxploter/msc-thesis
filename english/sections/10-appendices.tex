\section*{Appendices} \label{appendices} \addcontentsline{toc}{section}{Appendices}

\subsection*{A. Training Hyperparameters} \label{Appendix:TrainingHyperparameters}

\begin{table}[htb!]
    \centering
    \caption{Hyperparameters used for training the RPerceiver model for bounding box prediction task.}
    \label{tab:training_params_bbox_20250505}
    \begin{tblr}{width=1\textwidth, hlines, vlines,
                   colspec = { l l X },
                   row{1} = {font=\bfseries},
                   colsep=3pt,
                  }
        Hyperparameter & Value & Description \\
        backbone & cnn & Backbone type \\
        batch\_size & 1 & Batch size for training \\
        bbox\_loss\_coef & 5 & L1 box coefficient \\
        eos\_coef & 0.1 & Relative classification weight of the no-object class \\
        epochs & 32 & Number of training epochs \\
        giou\_loss\_coef & 2 & GIoU box coefficient \\
        learning\_rate & 0.0001 & Learning rate \\
        learning\_rate\_backbone & 0.0001 & Learning rate \\
        num\_frames & 20 & Number of frames. \\
        object\_detection & True & Use object detection prediction head \\
        resize\_frame & 320 & Resize frame to this size \\
        scheduler\_step\_size & 18,28 & Scheduler step size \\
        set\_cost\_bbox & 5 & L1 box coefficient in the matching cost \\
        set\_cost\_class & 1 & Class coefficient in the matching cost \\
        set\_cost\_giou & 2 & giou box coefficient in the matching cost \\
        weight\_decay & 0.01 & Weight decay for optimizer \\
        weight\_loss\_bce & 1 & Weight loss binary cross entropy \\
        weight\_loss\_center\_point & 5 & Weight loss center point \\
    \end{tblr}
\end{table}

\begin{table}[htb!]
    \centering
    \caption{Hyperparameters used for training the RPerceiverMM model for center point task.}
    \label{tab:training_params_cp_20250505}
    \begin{tblr}{width=1\textwidth, hlines, vlines,
                   colspec = { l l X },
                   row{1} = {font=\bfseries},
                   colsep=3pt,
                  }
        Hyperparameter & Value & Description \\
        backbone & cnn & Backbone type \\
        batch\_size & 1 & Batch size for training \\
        bbox\_loss\_coef & None & L1 box coefficient \\
        eos\_coef & None & Relative classification weight of the no-object class \\
        epochs & 21 & Number of training epochs \\
        giou\_loss\_coef & None & GIoU box coefficient \\
        learning\_rate & 0.0001 & Learning rate \\
        learning\_rate\_backbone & 0.0001 & Learning rate \\
        num\_frames & 12 & Number of frames. \\
        object\_detection & None & Use object detection prediction head \\
        resize\_frame & None & Resize frame to this size \\
        scheduler\_step\_size & 18 & Scheduler step size \\
        set\_cost\_bbox & None & L1 box coefficient in the matching cost \\
        set\_cost\_class & None & Class coefficient in the matching cost \\
        set\_cost\_giou & None & giou box coefficient in the matching cost \\
        weight\_decay & 0.01 & Weight decay for optimizer \\
        weight\_loss\_bce & 1 & Weight loss binary cross entropy \\
        weight\_loss\_center\_point & 5 & Weight loss center point \\
    \end{tblr}
\end{table}

\clearpage
\subsection*{B. Model Hyperparameters} \label{Appendix:ModelHyperparameters}

\begin{table}[htb!]
    \centering
    \caption{Hyperparameters used for configuring RPerceiver model.}
    \label{tab:model_params_RPerceiver_20250505}
    \begin{tblr}{width=1\textwidth, hlines, vlines,
                   colspec = { l l X },
                   row{1} = {font=\bfseries},
                   colsep=3pt,
                  }
        Hyperparameter & Value & Description \\
        enc\_layers & 1 & Number of layers in Perceiver encoder \\
        enc\_nheads\_cross & 1 & Number of cross-attention heads \\
        hidden\_dim & 128 & Latent dimension size \\
        max\_freq & 10 & Maximum frequency for Fourier encoding \\
        nheads & 1 & Number of latent self-attention heads \\
        num\_freq\_bands & 6 & Number of frequency bands for Fourier encoding \\
        num\_queries & 16 & Number of latents, or induced set points, or centroids \\
        self\_per\_cross\_attn & 1 & Number of self-attention blocks per cross-attention block \\
    \end{tblr}
\end{table}

\begin{table}[htb!]
    \centering
    \caption{Hyperparameters used for configuring RPerceiverMM model.}
    \label{tab:model_params_RPerceiverMM_20250505}
    \begin{tblr}{width=1\textwidth, hlines, vlines,
                   colspec = { l l X },
                   row{1} = {font=\bfseries},
                   colsep=3pt,
                  }
        Hyperparameter & Value & Description \\
        enc\_layers & 1 & Number of layers in Perceiver encoder \\
        enc\_nheads\_cross & 1 & Number of cross-attention heads \\
        hidden\_dim & 128 & Latent dimension size \\
        max\_freq & 10 & Maximum frequency for Fourier encoding \\
        nheads & 1 & Number of latent self-attention heads \\
        num\_freq\_bands & 6 & Number of frequency bands for Fourier encoding \\
        num\_queries & 16 & Number of latents, or induced set points, or centroids \\
        self\_per\_cross\_attn & 1 & Number of self-attention blocks per cross-attention block \\
    \end{tblr}
\end{table}

\subsection*{C. Train Procedures} \label{Appendix:TrainProcedures}

The Python code snippet below details the implementation of the \texttt{shuffle} and \texttt{dropout} procedures within the model's forward pass method. The \texttt{shuffle} mechanism, activated when \texttt{self.apply\_shuffle} is true, randomizes the processing order of sensor views (\texttt{view\_indices}) within each timestep using \texttt{get\_shuffled\_order}, mimicking potential real-world asynchronicity. The \texttt{dropout} strategy is applied conditionally based on the timestep: dropout is only considered for timesteps in the second half of the input sequence (i.e., where \texttt{timestep >= midpoint}, with \texttt{midpoint} being the sequence's halfway point). For these eligible timesteps, a probabilistic check (\texttt{should\_drop\_view(dropout\_probability)}) determines if the current view (\texttt{drop\_this\_view}) is actually dropped. When a view is dropped, the code bypasses both the feature extraction via the \texttt{backbone} and the \texttt{cross\_attention} step within the perceiver layers for that view. However, the \texttt{self\_attention} mechanism within each layer is executed unconditionally. The model's core state, represented by \texttt{latent\_array}, is updated iteratively within the loop processing each view: after every perceiver layer's self-attention step, the \texttt{latent\_array} is updated. If the view was not dropped, this update incorporates information fused via cross-attention before the self-attention step; otherwise, the self-attention refines the \texttt{latent\_array} based solely on the state propagated from processing the previous view. Finally, after processing all views for a timestep, a prediction is generated from the updated \texttt{latent\_array}.

\begin{minted}[tabsize=4, showtabs=true]{python}

def forward(self, sequence_of_frames, dropout_probability):
    latent_array = self.initial_learned_hidden_state
    all_predictions = []
    midpoint = len(sequence_of_frames) // 2
    for timestep, frame_views in enumerate(sequence_of_frames):
        should_apply_dropout = (timestep >= midpoint)

        view_indices = list(range(len(frame_views)))
        if self.apply_shuffle:
            view_indices = get_shuffled_order(view_indices)

        for view_index in view_indices:
            view_data = frame_views[view_index]
            drop_this_view = should_apply_dropout
            if drop_this_view:
                drop_this_view = should_drop_view(dropout_probability)

            extracted_features = None
            if not drop_this_view:
                extracted_features = self.backbone(view_data)

            for perceiver_layer in self.perceiver.layers:
                if not drop_this_view:
                    latent_array = perceiver_layer.cross_attention(
                        q=latent_array,
                        kv=extracted_features,
                        sensor_id=view_index,
                    )

                latent_array = perceiver_layer.self_attention(
                    qkv=latent_array,
                )

        prediction = self.detection_head(latent_array)
        all_predictions.append(prediction)

    return all_predictions

\end{minted}
