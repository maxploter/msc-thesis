\section{Conclusion} \label{Conclusion}
This thesis investigated the performance of video object detection models, specifically focusing on their robustness under degraded sensor input scenarios, a critical challenge for autonomous driving systems (ADS). We introduced two novel architectures: the Recurrent Perceiver (RPerceiver) and its multi-modal variant (RPerceiverMM), inspired by the Perceiver architecture's ability to handle high-dimensional, multi-modal data but adapted for sequential inputs like video[cite: 130, 133, 139].

To facilitate this research, a custom dataset, "detection-moving-mnist-easy," was generated, providing controlled video sequences with annotations for evaluating both bounding box and center point prediction tasks[cite: 163, 166, 177]. We compared the RPerceiver against a strong baseline, YOLOv8n, on the bounding box detection task[cite: 262, 277]. While YOLOv8n performed better on overlapping objects, the RPerceiver demonstrated superior performance in handling objects near frame borders, suggesting its effectiveness in leveraging temporal information[cite: 283, 284]. Notably, RPerceiver achieved competitive mAP scores with significantly fewer parameters and lower computational cost compared to YOLOv8n[cite: 278, 279].

A core contribution of this work was the investigation of training strategies to enhance model resilience against sensor input degradation, simulating real-world ADS challenges like sensor failure and asynchronous data arrival[cite: 178, 179, 185, 186]. We introduced and evaluated 'dropout' and 'shuffle' training procedures[cite: 187, 189, 192]. Ablation studies using the center point prediction task demonstrated the efficacy of these strategies. In single-view scenarios, training RPerceiver with dropout (RP (d)) resulted in significantly better performance under simulated complete sensor failure (the 'blind' evaluation procedure) compared to the baseline model, although it incurred a minor performance penalty under normal conditions[cite: 302, 307, 310, 312, 315, 316].

Extending the analysis to multi-view scenarios using RPerceiverMM, we further confirmed the benefits of targeted training procedures. Models trained with input shuffling (RPMM (s)) excelled when evaluated with shuffled sensor inputs, while dropout-trained models (RPMM (d)) showed the best robustness against simulated sensor failure ('blind' evaluation)[cite: 323, 326, 331]. The model trained with both dropout and shuffling (RPMM (d, s)) provided the most robust performance under the combined 'blind, shuffle' evaluation, highlighting the potential for tailoring training methods to specific anticipated failure modes[cite: 332]. While these robustness-enhancing training procedures introduced a slight performance decrease under default, non-degraded conditions, they offer substantial improvements in resilience against common sensor issues faced by ADS[cite: 330].

In summary, this research successfully developed and evaluated recurrent Perceiver-based architectures for video object detection, demonstrating their potential, particularly in leveraging temporal information for border cases. Furthermore, the investigation into dropout and shuffle training procedures provides valuable insights into enhancing model robustness against sensor degradation, offering promising strategies for building more reliable perception systems for autonomous vehicles. Future work could explore the application of these models and training techniques on more complex, real-world datasets, investigate alternative methods for handling multi-modality, and further refine the architecture for improved efficiency and accuracy.
