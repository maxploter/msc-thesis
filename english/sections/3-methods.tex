\section{Methods}  \label{Methods}

In this section We propose a novel RNN architecture called the Recurrent Perceiver to model an object's changing appearance and motion over time for video object detection.
This section outlines the procedure used for training the models presented in this thesis.

\subsection{Model} \label{Methods:Model}

We introduce a new Recurrent Perceiver (RPerceiver), a Recurrent Neural Network (RNN) capable of processing high-dimensional inputs. This architecture is inspired by the Perceiver \cite{jaeglePerceiverGeneralPerception2021} due to its ability to handle high-dimensional data.
We have re-engineered the Perceiver architecture by adding a temporal dimension, effectively unrolling it over time. This addresses the original Perceiver's limitation of producing only a single output per input, which makes it unsuitable for tasks with a temporal component, such as video object detection. If previously, the Perceiver was only unrolled in depth, now we have closed the loop and unrolled it in time by propagating the latent array between time steps. For the initial time step, we still use a learnable latent array, similar to the original Perceiver \cite{jaeglePerceiverGeneralPerception2021}.
The architecture of the RPerceiver is illustrated in Figure \ref{fig:figure_methods_recurrent_perceiver}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_recurrent_perceiver.png}
    \caption{The Recurrent Perceiver (RPerceiver) architecture. The model processes inputs along the time dimension by propagating the latent array between time steps. In the depth dimension (within a single time step), this example shows two processing blocks, similar to the original Perceiver, applied to the input and latent arrays.}
    \label{fig:figure_methods_recurrent_perceiver}
\end{figure}

% TODO: Read about calibration problem.

We propose a variation of the RPerceiver architecture capable of processing multi-modal inputs, which we call the Recurrent Perceiver Multi-Modal (RPerceiverMM). The original Perceiver paper \cite{jaeglePerceiverGeneralPerception2021} handled multi-modality by concatenating a learned, modality-specific encoding to each input element. Modern Autonomous Driving Systems (ADS) process information from multiple sensors, often including multiple cameras placed in different locations, which presents a calibration challenge. Therefore, our work adopts a different approach to multi-modality. We introduce a camera-specific cross-attention module to the RPerceiverMM. Within the scope of this thesis, we focus on a multi-view camera setup. The architecture of the RPerceiverMM is illustrated in Figure~\ref{fig:figure_methods_recurrent_perceiver_mm}.

% TODO edit fiture and name

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_recurrent_perceiver_mm.png}
    \caption{The Recurrent Perceiver Multi-Modal (RPerceiverMM) architecture designed for multi-view camera inputs. It employs camera-specific cross-attention modules to process information from different sensors (views). In this example, separate cross-attention blocks are used for each camera input, integrating modality-specific information.}
    \label{fig:figure_methods_recurrent_perceiver_mm}
\end{figure}

In addition to the RPerceiver module, we used a CNN backbone to learn a 2D representation of the input frames. We designed this backbone consisting of four blocks with convolutional layers and ReLU activation. Each block progressively downscales spatial dimensions by a factor of 2 while increasing channel dimensionality, resulting in a final feature map with 128 channels and $\frac{1}{16}$ of the original spatial resolution.
The output features from the RPerceiver module are passed to the detection heads, which are responsible for predicting object labels and positions. We employed detection heads the same as those in the DETR model \cite{carionEndtoEndObjectDetection2020}. These heads include a linear layer to predict the class label using a softmax function and a 3-layer Multi-Layer Perceptron (MLP) with ReLU activation function to predict object coordinates. We considered two variants for object coordinates: (i) bounding boxes, where the MLP predicts the normalized center coordinates, height, and width relative to the input image, and (ii) center points. The corresponding output dimensions are $N \times 4$ and $N \times 2$, respectively, where $N$ is a fixed number of detection slots (queries). Since $N$ is typically much larger than the actual number of objects, a special class label $\emptyset$ is used to indicate that no object is detected within a slot, playing a role similar to a background class.
For bounding boxes, we used a sigmoid activation function to predict the normalized coordinates of the bounding box center, as well as its width and height. For center points, we used a tanh activation function. This choice was made because we placed the origin of the coordinate system at the center to mimic a bird's-eye view, similar to an ADS positioned centrally with surrounding views.
The complete architecture of the model is shown in Figure~\ref{fig:figure_methods_recurrent_perceiver_complete}.

% TODO edit fiture and name

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_recurrent_perceiver_complete.png}
    \caption{Full architecture of our model. The backbone output is supplemented with a learned view-specific positional encoding before passing it to the autoregressive Perceiver model. The autoregressive Perceiver takes a hidden state from the previous frame's last view and iterates through the current frame's views. After the last view, we pass the output embedding of the Perceiver to the feed-forward network (FFN) that predicts class labels and center points on the global frame raster.}
    \label{fig:figure_methods_recurrent_perceiver_complete}
\end{figure}


\subsection{Dataset} \label{Methods:Dataset}

For our experiment, we generated our own dataset, which we call "detection-moving-mnist-easy". We took inspiration from the MovingMNIST dataset \cite{srivastava2016unsupervisedlearningvideorepresentations}, which is used for TODO use cases. In our case, we are interested in video object detection and a simplified variation of keypoints, where we predict a center point of the object it is similar to keypoints task because the center of object is not the same as center of the bounding box. We hosted the dataset on Hugging Face \footnote{\url{https://huggingface.co/datasets/Max-Ploter/detection-moving-mnist-easy}}.

 For the first frame, we pick a number of digits from 1 to 10 with uniform probability (see Figure~\ref{fig:figure_method_dataset_train_digit_classes}). Depending on the number of digits per first frame, we draw, without replacement, from the well-known MNIST dataset \cite{} (from the train and test splits corresponding to the dataset split). Each digit is placed on the first frame of the canvas image of size 128x128. There's a greedy algorithm that places digits on the first frame randomly and tries to avoid overlaps, so it's easier for the model to detect all objects in the beginning. To each digit, we assign an affine translation from -5 to 5 randomly with uniform probability. Then, we apply corresponding affine transformations to move the digits through 20 frames on the canvas image of size 128x128. As a result, we receive a tensor of size 20x1x128x128, which represents a video (see Figure~\ref{fig:figure_methods_dataset_detection_mmnist_sequence}).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_method_dataset_train_digit_classes.png}
    \caption{Distribution of classes in the "detection-moving-mnist-easy" dataset.}
    \label{fig:figure_method_dataset_train_digit_classes}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_dataset_detection_mmnist_sequence.png}
    \caption{Example of 12 frames from the sequence. Ground truth, shown in red, indicates the ground truth digit center point and a class label.}
    \label{fig:figure_methods_dataset_detection_mmnist_sequence}
\end{figure}


In order for dataset to be more challanging we do not restrict digit overlap in subsequent frames. It is even possible to have some degree of overlap in the first frame if the greedy algorithm is unable to randomly place digits in such a way on the first frame. We do not bounce digits against image boundaries, so each digit can leave the frame. You can see in the figure that later frames have fewer digits (see Figure~\ref{fig:figure_method_dataset_train_digits_per_frame}).

% TODO DOUBLE CHECK number this plot
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_method_dataset_train_digits_per_frame.png}
    \caption{Distribution of classes in the "detection-moving-mnist-easy" dataset.}
    \label{fig:figure_method_dataset_train_digits_per_frame}
\end{figure}

We generated the dataset with 60K and 10K train and test splits, respectively. Annotations, automatically generated during sequence creation, include digit classes and digit center point coordinates (keypoint), bounding box coordinates and digit's identity ID.

\subsection{Dropout and Shuffle} \label{Methods:DropoutAndShuffle}

As previously discussed in Section \ref{Background:AutonomousDrivingSystems}, ADSs face challenges related to sensor reliability. Two critical issues identified are the potential for complete sensor failure, where a sensor modality ceases to provide any data, and the non-deterministic availability of sensor inputs. To enhance the resilience of perception models against these real-world imperfections, this work investigates training strategies designed to improve model robustness. Specifically, we aim to experimentally evaluate how models perform when subjected to simulated conditions mimicking both complete sensor data loss (akin to sensor failure) and non-deterministic input sequences (akin to middleware-induced timing variations). The following training procedures, \texttt{dropout} and \texttt{shuffle}, are introduced to directly address these challenges during the model training phase. A model trained without applying these training procedures is considered the baseline.

\begin{description}
    \item[\texttt{shuffle}] In this procedure, the sensor inputs are randomly permuted within each time step. Consequently, the model receives inputs from the sensors in a random order for that specific time step. This shuffling only occurs for sensor inputs within the same time step and aims to simulate the non-deterministic ordering of inputs that can occur due to middleware asynchronicity. This training procedure is only applicable to a multi-view setup.

    \item[\texttt{dropout}] This procedure simulates scenarios where sensor information is missing, directly addressing the challenge of potential complete sensor failure for one or more modalities. To achieve this, we train the model by intermittently dropping sensor inputs (input dropout). We keep the first half of the input sequence intact (no dropout), allowing the model to accumulate features in its hidden state. The second half of the sequence may undergo frame dropout depending on the dropout probability. During training, we gradually increase the probability of an information dropout from 10\% up to 86.6\%.
\end{description}

The Python code snippet below details the implementation of the \texttt{shuffle} and \texttt{dropout} procedures within the model's forward pass method. The \texttt{shuffle} mechanism, activated when \texttt{self.apply\_shuffle} is true, randomizes the processing order of sensor views (\texttt{view\_indices}) within each timestep using \texttt{get\_shuffled\_order}, mimicking potential real-world asynchronicity. The \texttt{dropout} strategy is applied conditionally based on the timestep: dropout is only considered for timesteps in the second half of the input sequence (i.e., where \texttt{timestep >= midpoint}, with \texttt{midpoint} being the sequence's halfway point). For these eligible timesteps, a probabilistic check (\texttt{should\_drop\_view(dropout\_probability)}) determines if the current view (\texttt{drop\_this\_view}) is actually dropped. When a view is dropped, the code bypasses both the feature extraction via the \texttt{backbone} and the \texttt{cross\_attention} step within the perceiver layers for that view. However, the \texttt{self\_attention} mechanism within each layer is executed unconditionally. The model's core state, represented by \texttt{latent\_array}, is updated iteratively within the loop processing each view: after every perceiver layer's self-attention step, the \texttt{latent\_array} is updated. If the view was not dropped, this update incorporates information fused via cross-attention before the self-attention step; otherwise, the self-attention refines the \texttt{latent\_array} based solely on the state propagated from processing the previous view. Finally, after processing all views for a timestep, a prediction is generated from the updated \texttt{latent\_array}.

\begin{minted}[tabsize=4, showtabs=true]{python}

def forward(self, sequence_of_frames, dropout_probability):
    latent_array = self.initial_learned_hidden_state
    all_predictions = []
    midpoint = len(sequence_of_frames) // 2
    for timestep, frame_views in enumerate(sequence_of_frames):
        should_apply_dropout = (timestep >= midpoint)

        view_indices = list(range(len(frame_views)))
        if self.apply_shuffle:
            view_indices = get_shuffled_order(view_indices)

        for view_index in view_indices:
            view_data = frame_views[view_index]
            drop_this_view = should_apply_dropout
            if drop_this_view:
                drop_this_view = should_drop_view(dropout_probability)

            extracted_features = None
            if not drop_this_view:
                extracted_features = self.backbone(view_data)

            for perceiver_layer in self.perceiver.layers:
                if not drop_this_view:
                    latent_array = perceiver_layer.cross_attention(
                        q=latent_array,
                        kv=extracted_features,
                        sensor_id=view_index,
                    )

                latent_array = perceiver_layer.self_attention(
                    qkv=latent_array,
                )

        prediction = self.detection_head(latent_array)
        all_predictions.append(prediction)

    return all_predictions

\end{minted}


\subsection{Loss Function} \label{Methods:LossFunction}

We adopted the loss calculation approach from DETR \cite{carionEndtoEndObjectDetection2020} and \cite{stewartEndtoendPeopleDetection2015}. The model (\ref{Methods:Model}) predicts a fixed-size set of $N$ potential objects per timestep. The parameter $N$ is a dimension of the latent array and is chosen to be greater than the cardinality of the largest set of ground-truth objects per frame. The model uses an attention mechanism that allows it to attend to all other elements in the attention map. Therefore, there is no predefined structure, like a grid in one-stage image detectors \cite{redmonYouOnlyLook2016}, associating predictions with ground-truth objects. This presents the core challenge of how to score the fixed-size prediction set against the variable-size ground-truth set. The loss function calculation consists of two steps: first, finding an optimal bipartite matching between the elements of the predicted and ground-truth sets, and second, calculating a loss.

Let us denote the set of predictions for a given timestep as $ \hat{Y} = \{\hat{y}_j\}_{j=1}^N $ and the corresponding set of ground-truth objects for that frame as $ Y = \{y_i\}_{i=1}^M $ where $ N > M $. At each recurrence (i.e., for each frame/timestep processed), the RPerceiver outputs the entire set of $N$ predictions $\hat{Y}$. Each prediction $\hat{y}_j \in \hat{Y}$ consists of a predicted object position $\hat{o}_j$ and a predicted class label $\hat{c}_j$ (including the possibility of the no-object class $\emptyset$). The position $\hat{o}_j$ can represent different objects depending on the specific task:
\begin{itemize}
    \item For the \textbf{bounding box prediction task}, $\hat{o}_j$ is the predicted box $\hat{b}_j = (\hat{b}_x, \hat{b}_y, \hat{b}_w, \hat{b}_h) \in \mathbb{R}^4$, representing center coordinates, height, and width relative to the frame size.
    \item For the \textbf{center point prediction task}, $\hat{o}_j$ is the predicted center point coordinates $\hat{p}_j = (\hat{p}_x, \hat{p}_y) \in \mathbb{R}^2$ relative to the frame size.
\end{itemize}
Similarly, each ground-truth object $y_i \in Y$ consists of the ground-truth object position $o_i$ (either a box $b_i$ or a point $p_i$) and the ground-truth class label $c_i$.
We define a matching algorithm via an injective function $f: Y \rightarrow \hat{Y}$, where $f(y_i)$ is the candidate hypothesis $\hat{y}_j \in \hat{Y}$ assigned to the ground-truth object $y_i$. Given $f$, we define a loss function on pairs of sets $Y$ and $\hat{Y}$ as \cite{stewartEndtoendPeopleDetection2015}:

\begin{equation} \label{eq:set_loss}
\mathcal{L}(Y, \hat{Y}, f ) = \sum_{i=1}^{M} \mathcal{L}_{pos}(y_i, f(y_i)) +  \sum_{j=1}^{N} \mathcal{L}_c (\hat{y}_j, f^{-1}(\hat{y}_j))
\end{equation}

where $\mathcal{L}_{pos}$ is the object localization loss. The specific formulation of $\mathcal{L}_{pos}$ depends on the task (bounding box or center point). $\mathcal{L}_c$ is the class prediction loss, for which we used Focal Loss \cite{linFocalLossDense2018}. $f^{-1}(\cdot)$ is an inverse of the matching function $f$:

\begin{equation*}
f^{-1}(\hat{y}_j) =
\begin{cases}
y_i & \exists \, y_i \in Y, f(y_i) = \hat{y}_j \\
\emptyset & \text{otherwise} \\
\end{cases}
\end{equation*}
Details on each loss term are provided below.

% TODO Mention hyperparameters
\textbf{Focal Loss.} The Focal Loss \cite{linFocalLossDense2018} is used to address the class imbalance between the foreground objects and the numerous potential background predictions. The class loss component $\mathcal{L}_c$ for each prediction $\hat{y}_j$ involves the sigmoid Focal Loss. The total class loss for a prediction $\hat{y}_j$ is summed over all foreground classes $k \in \{1, ..., C\}$, where $C$ is the number of object categories (excluding the $\emptyset$ class). The Focal Loss for a single class $k$ and prediction $j$ is defined using the $\alpha$-balanced form:
\begin{equation} \label{eq:focal_loss_corrected}
    \text{FL}(p_{jk}, y_{jk}) = - \alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}
where $p_{jk} = \sigma(x_{jk})$ is the predicted sigmoid probability for class $k$ derived from the raw logits $x_{jk}$, and $y_{jk}$ is the ground-truth label (1 if the matched ground-truth object $f^{-1}(\hat{y}_j)$ belongs to class $k$, and 0 otherwise). The terms $p_t$ and $\alpha_t$ depend on the ground-truth label $y_{jk}$:
\begin{align*}
    p_t &= 
    \begin{cases} 
        p_{jk} & \text{if } y_{jk} = 1 \\
        1 - p_{jk} & \text{if } y_{jk} = 0 
    \end{cases} \\
    \alpha_t &= 
    \begin{cases} 
        \alpha & \text{if } y_{jk} = 1 \\
        1 - \alpha & \text{if } y_{jk} = 0 
    \end{cases}
\end{align*}
The term $\gamma \ge 0$ is the focusing parameter, which reduces the relative loss for well-classified examples ($p_t \to 1$), thereby putting more focus on hard, misclassified examples. The term $\alpha \in [0, 1]$ is a weighting factor to address class imbalance, typically set as a hyperparameter (e.g., $\alpha=0.25$). The total class loss contribution for prediction $j$ in Equation \ref{eq:set_loss} is calculated as:
\begin{equation*}
    \mathcal{L}_c(\hat{y}_j, y_j^{\text{match}}) = \sum_{k=1}^{C} \text{FL}(p_{jk}, y_{jk}) 
\end{equation*}
For unmatched predictions ($\hat{y}_j$ such that $y_j^{\text{match}} = 0$), the ground truth is $y_{jk}=0$ for all foreground classes $k$, and the loss calculation correctly handles them as background/$\emptyset$ class predictions.

\textbf{Bounding Box Loss} is a linear combination of the L1 loss and the Generalized IoU (GIoU) loss \cite{rezatofighiGeneralizedIntersectionUnion2019}:
\begin{equation}
    \mathcal{L}_{box}(b_i, \hat{b}_j) = \lambda_{L1} \mathcal{L}_{L1}(b_i, \hat{b}_j) + \lambda_{giou} \mathcal{L}_{giou}(b_i, \hat{b}_j)
    \label{eq:box_loss_revised}
\end{equation}
where $y_i = (c_i, b_i)$ and $\hat{y}_j = (\hat{c}_j, \hat{b}_j)$. Loss terms $ \mathcal{L}_{L1}, \mathcal{L}_{giou} $ are weighted by hyperparameters $ \lambda_{L1}, \lambda_{giou} \in \mathbb{R} $.

Both the $ \mathcal{L}_{L1} $ and $ \mathcal{L}_{giou} $ components are normalized by the total number of actual ground-truth boxes ($M$) across the batch. Thus, for matched pairs in the bounding box task, $l_{pos}(y_i, \hat{y}_j) = \mathcal{L}_{box}(b_i, \hat{b}_j)$.

\textbf{Center Point Loss.} is L1 distance between the ground-truth center point $p_i$ and the predicted center point $\hat{p}_j$:
\begin{equation} \label{eq:point_loss}
    \mathcal{L}_{point}(p_i, \hat{p}_j) = \mathcal{L}_{L1}(p_i, \hat{p}_j)
\end{equation}
where $y_i = (c_i, p_i)$ and $\hat{y}_j = (\hat{c}_j, \hat{p}_j)$. The total localization loss term in Equation \ref{eq:set_loss} for this task involves summing $\mathcal{L}_{point}$ over all matched pairs and normalizing by the total number of ground-truth objects ($M$) in the batch. Thus, for matched pairs in the center point task, $\mathcal{L}_{pos}(y_i, \hat{y}_j) = \mathcal{L}_{point}(p_i, \hat{p}_j)$.

\textbf{Hungarian loss.} We use comparison cost function $\mathcal{L}_{match}: Y x \hat{Y} \rightarrow \mathbb{R}$ between hypotheses and ground-truth object. The comparison cost function $\mathcal{L}_{match}(y_i, \hat{y}_j)$ depends on the task. For the \textbf{bounding box task}, it is defined as \cite{carionEndtoEndObjectDetection2020}:
\begin{equation} \label{eq:matching_cost_bbox}
\mathcal{L}_{match}^{\text{bbox}}(y_i, \hat{y}_j) = - \mathbb{I}\{c_i \neq \emptyset\} \hat{p}_{j}(c_i) + \mathbb{I}\{c_i \neq \emptyset\} \mathcal{L}_{box}(b_i, \hat{b}_{j})
\end{equation}
where $\hat{p}_j(c_i)$ is the predicted probability of the ground-truth class $c_i$ for prediction $j$. For the \textbf{center point task}, the cost replaces $\mathcal{L}_{box}$ with the appropriate point localization cost $\mathcal{L}_{point}$:
\begin{equation} \label{eq:matching_cost_point}
\mathcal{L}_{match}^{\text{point}}(y_i, \hat{y}_j) = - \mathbb{I}\{c_i \neq \emptyset\} \hat{p}_{j}(c_i) + \mathbb{I}\{c_i \neq \emptyset\} \mathcal{L}_{point}(p_i, \hat{p}_{j})
\end{equation}

Given the definition of comparison cost function $\mathcal{L}_{match}$, we find optimal cost bipartite matching between $ Y $ and $ \hat{Y} $ efficiently using the Hungarian algorithm \cite{kuhnHungarianMethodAssignment1955}. The function $f$ used in Equation \ref{eq:set_loss} is then defined by this optimal assignment. The overall loss computed using this optimal matching is referred to as the Hungarian loss, $\mathcal{L}_{Hungarian}(Y, \hat{Y}) = \mathcal{L}(Y, \hat{Y}, f_{Hungarian})$.

\subsection{Metrics} \label{Methods:Metrics}

We used mean Average Precision (mAP) as the primary metric to evaluate the model's object detection performance. A widely used metric in object detection, mAP measures the average precision across various Intersection over Union (IoU) thresholds, adapted from information retrieval evaluation methods and popularized in challenges like PASCAL VOC \cite{everinghamPascalVisualObject2010}. Specifically, we report the mAP at an IoU threshold of 0.5 (mAP@0.5), a common choice established in early object detection benchmarks such as PASCAL VOC \cite{everinghamPascalVisualObject2010}. We also report the mAP at an IoU threshold of 0.75 (mAP@0.75), which provides a stricter evaluation criterion. Additionally, we include the mAP averaged over IoU thresholds ranging from 0.5 to 0.95 with a step of 0.05 (mAP@0.5:0.95), as introduced by the COCO challenge \cite{linMicrosoftCOCOCommon2015a}.

For evaluating the model's center point prediction performance, we used the Average Displacement Error (ADE) and Final Displacement Error (FDE) metrics. The ADE measures the average Euclidean distance between the predicted and ground truth center points over the sequence of frames, as shown in Equation \ref{eq:ade}.

\begin{equation}
    \text{ADE} = \frac{\sum_{t=1}^{T} \sum_{i=1}^{M_t} || \pi_{\text{pos}}(f_t(y_{i,t})) - \pi_{\text{pos}}(y_{i,t}) ||_2}{\sum_{t=1}^{T} M_t}
    \label{eq:ade}
\end{equation}

where:
\begin{itemize}
    \item $T$ is the total number of time steps (frames) in the sequence.
    \item $M_t$ is the number of ground truth objects $y_{i,t}$ present at time step $t$.
    \item $y_{i,t}$ is the ground truth object (containing class and position $p_{i,t}$) at time $t$.
    \item $f_t$ is the matching function at time $t$ mapping ground truth objects $Y_t$ to predictions $\hat{Y}_t$ and introduced in \ref{Methods:LossFunction}.
    \item $\pi_{\text{pos}}(\cdot)$ is a function extracting the center point position coordinates from its argument (e.g., $\pi_{\text{pos}}(y_{i,t}) = p_{i,t}$ and $\pi_{\text{pos}}(f_t(y_{i,t}))$ extracts the coordinates $\hat{p}$ from the matched prediction $\hat{y}=f_t(y_{i,t})$).
\end{itemize}


The FDE measures the Euclidean distance between the predicted and ground truth center points only at the final time step ($T$) of the sequence, normalized by the number of ground truth center points, as defined in Equation \ref{eq:fde}.

\begin{equation}
    \text{FDE} = \frac{1}{M_T} \sum_{i=1}^{M_T} || \pi_{\text{pos}}(f_T(y_{i,T})) - \pi_{\text{pos}}(y_{i,T}) ||_2
    \label{eq:fde}
\end{equation}

where:
\begin{itemize}
    \item $T$ is the final time step of the sequence being evaluated.
    \item $M_T$ is the number of ground truth objects $y_{i,T}$ present at the final time step $T$. (Note: This calculation assumes $M_T > 0$).
    \item $y_{i,T}$ is the $i$-th ground truth object at the final time step $T$.
    \item $f_T$ is the matching function specific to the final time step $T$, mapping ground truths $Y_T$ to predictions $\hat{Y}_T$ and introduced in \ref{Methods:LossFunction}.
    \item $\pi_{\text{pos}}(\cdot)$ is the function extracting the center point position coordinates from its argument.
\end{itemize}
