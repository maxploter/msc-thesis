\section{Methods}  \label{Methods}

In this section We propose a novel RNN architecture called the Recurrent Perceiver to model an object's changing appearance and motion over time for video object detection.
This section outlines the procedure used for training the models presented in this thesis.

\subsection{Model} \label{Methods:Model}

We introduce a new Recurrent Perceiver (RPerceiver), a Recurrent Neural Network (RNN) capable of processing high-dimensional inputs. This architecture is inspired by the Perceiver \cite{jaeglePerceiverGeneralPerception2021} due to its ability to handle high-dimensional data.
We have re-engineered the Perceiver architecture by adding a temporal dimension, effectively unrolling it over time. This addresses the original Perceiver's limitation of producing only a single output per input, which makes it unsuitable for tasks with a temporal component, such as video object detection. If previously, the Perceiver was only unrolled in depth, now we have closed the loop and unrolled it in time by propagating the latent array between time steps. For the initial time step, we still use a learnable latent array, similar to the original Perceiver \cite{jaeglePerceiverGeneralPerception2021}.
The architecture of the RPerceiver is illustrated in Figure \ref{fig:figure_methods_recurrent_perceiver}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_recurrent_perceiver.png}
    \caption{The Recurrent Perceiver (RPerceiver) architecture. The model processes inputs along the time dimension by propagating the latent array between time steps. In the depth dimension (within a single time step), this example shows two processing blocks, similar to the original Perceiver, applied to the input and latent arrays.}
    \label{fig:figure_methods_recurrent_perceiver}
\end{figure}

% TODO: Read about calibration problem.

We propose a variation of the RPerceiver architecture capable of processing multi-modal inputs, which we call the Recurrent Perceiver Multi-Modal (RPerceiverMM). The original Perceiver paper \cite{jaeglePerceiverGeneralPerception2021} handled multi-modality by concatenating a learned, modality-specific encoding to each input element. Modern Autonomous Driving Systems (ADS) process information from multiple sensors, often including multiple cameras placed in different locations, which presents a calibration challenge. Therefore, our work adopts a different approach to multi-modality. We introduce a camera-specific cross-attention module to the RPerceiverMM. Within the scope of this thesis, we focus on a multi-view camera setup. The architecture of the RPerceiverMM is illustrated in Figure~\ref{fig:figure_methods_model_ar_perceiver_views}.

% TODO edit fiture and name

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_recurrent_perceiver_mm.png}
    \caption{The Recurrent Perceiver Multi-Modal (RPerceiverMM) architecture designed for multi-view camera inputs. It employs camera-specific cross-attention modules to process information from different sensors (views). In this example, separate cross-attention blocks are used for each camera input, integrating modality-specific information.}
    \label{fig:figure_methods_recurrent_perceiver_mm}
\end{figure}

In addition to the RPerceiver module, we used a CNN backbone to learn a 2D representation of the input frames. We designed this backbone consisting of four blocks with convolutional layers and ReLU activation. Each block progressively downscales spatial dimensions by a factor of 2 while increasing channel dimensionality, resulting in a final feature map with 128 channels and $\frac{1}{16}$ of the original spatial resolution.
The output features from the RPerceiver module are passed to the detection heads, which are responsible for predicting object labels and positions. We employed detection heads the same as those in the DETR model \cite{carionEndtoEndObjectDetection2020}. These heads include a linear layer to predict the class label using a softmax function and a 3-layer Multi-Layer Perceptron (MLP) with ReLU activation function to predict object coordinates. We considered two variants for object coordinates: (i) bounding boxes, where the MLP predicts the normalized center coordinates, height, and width relative to the input image, and (ii) center points. The corresponding output dimensions are $N \times 4$ and $N \times 2$, respectively, where $N$ is a fixed number of detection slots (queries). Since $N$ is typically much larger than the actual number of objects, a special class label $\emptyset$ is used to indicate that no object is detected within a slot, playing a role similar to a background class.
For bounding boxes, we used a sigmoid activation function to predict the normalized coordinates of the bounding box center, as well as its width and height. For center points, we used a tanh activation function. This choice was made because we placed the origin of the coordinate system at the center to mimic a bird's-eye view, similar to an ADS positioned centrally with surrounding views.
The complete architecture of the model is shown in Figure~\ref{fig:figure_methods_model_r_perceiver_complte}.

% TODO edit fiture and name

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_model_ar_perceiver_views.png}
    \caption{Full architecture of our model. The backbone output is supplemented with a learned view-specific positional encoding before passing it to the autoregressive Perceiver model. The autoregressive Perceiver takes a hidden state from the previous frame's last view and iterates through the current frame's views. After the last view, we pass the output embedding of the Perceiver to the feed-forward network (FFN) that predicts class labels and center points on the global frame raster.}
    \label{fig:figure_methods_model_r_perceiver_complte}
\end{figure}


\subsection{Dataset} \label{Methods:Dataset}

For our experiment, we generated our own dataset, which we call "detection-moving-mnist-easy". We took inspiration from the MovingMNIST dataset \cite{srivastava2016unsupervisedlearningvideorepresentations}, which is used for TODO use cases. In our case, we are interested in video object detection and a simplified variation of keypoints, where we predict a center point of the object it is similar to keypoints task because the center of object is not the same as center of the bounding box. We hosted the dataset on Hugging Face \footnote{\url{https://huggingface.co/datasets/Max-Ploter/detection-moving-mnist-easy}}.

 For the first frame, we pick a number of digits from 1 to 10 with uniform probability (see Figure~\ref{fig:figure_method_dataset_train_digit_classes}). Depending on the number of digits per first frame, we draw, without replacement, from the well-known MNIST dataset \cite{} (from the train and test splits corresponding to the dataset split). Each digit is placed on the first frame of the canvas image of size 128x128. There's a greedy algorithm that places digits on the first frame randomly and tries to avoid overlaps, so it's easier for the model to detect all objects in the beginning. To each digit, we assign an affine translation from -5 to 5 randomly with uniform probability. Then, we apply corresponding affine transformations to move the digits through 20 frames on the canvas image of size 128x128. As a result, we receive a tensor of size 20x1x128x128, which represents a video (see Figure~\ref{fig:figure_methods_dataset_detection_mmnist_sequence}).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_method_dataset_train_digit_classes.png}
    \caption{Distribution of classes in the "detection-moving-mnist-easy" dataset.}
    \label{fig:figure_method_dataset_train_digit_classes}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_methods_dataset_detection_mmnist_sequence.png}
    \caption{Example of 12 frames from the sequence. Ground truth, shown in red, indicates the ground truth digit center point and a class label.}
    \label{fig:figure_methods_dataset_detection_mmnist_sequence}
\end{figure}


In order for dataset to be more challanging we do not restrict digit overlap in subsequent frames. It is even possible to have some degree of overlap in the first frame if the greedy algorithm is unable to randomly place digits in such a way on the first frame. We do not bounce digits against image boundaries, so each digit can leave the frame. You can see in the figure that later frames have fewer digits (see Figure~\ref{fig:figure_method_dataset_train_digits_per_frame}).

% TODO DOUBLE CHECK number this plot
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_method_dataset_train_digits_per_frame.png}
    \caption{Distribution of classes in the "detection-moving-mnist-easy" dataset.}
    \label{fig:figure_method_dataset_train_digits_per_frame}
\end{figure}

We generated the dataset with 60K and 10K train and test splits, respectively. Annotations, automatically generated during sequence creation, include digit classes and digit center point coordinates (keypoint), bounding box coordinates and digit's identity ID.

\subsection{Metrics} \label{Methods:Metrics}

% TODO: mention mAP 75

We used mean Average Precision (mAP) as the primary metric to evaluate the model's performance. A widely used metric in object detection, mAP measures the average precision across various Intersection over Union (IoU) thresholds, adapted from information retrieval evaluation methods and popularized in challenges like PASCAL VOC \cite{everinghamPascalVisualObject2010}. Specifically, we report the mAP at an IoU threshold of 0.5, a common choice established in early object detection benchmarks such as PASCAL VOC \cite{everinghamPascalVisualObject2010}, and the mAP at 0.5-0.95, which represents the mean of the average precision calculated at IoU thresholds ranging from 0.5 to 0.95 with a step of 0.05, as introduced by the COCO challenge \cite{linMicrosoftCOCOCommon2015a}.

We used the Average Displacement Error (ADE) and Final Displacement Error (FDE) metrics to evaluate the model's performance. The ADE measures the average distance between the predicted and ground truth center points over the sequence of frames \ref{eq:ade}.

\begin{equation}
    \text{ADE} = \frac{1}{N \cdot T} \sum_{i=1}^{N} \sum_{t=1}^{T} || \hat{y}_{i,t} - y_{i,t} ||_2
    \label{eq:ade}
\end{equation}

The FDE measures the distance between the predicted and ground truth center points at the last frame of the sequence \ref{eq:fde}.

\begin{equation}
    \text{FDE} = \frac{1}{N} \sum_{i=1}^{N} || \hat{y}_{i,T} - y_{i,T} ||_2
    \label{eq:fde}
\end{equation}

\subsection{Loss Function} \label{Methods:LossFunction}

We adopted the loss calculation approach from DETR \cite{carionEndtoEndObjectDetection2020} and \cite{stewartEndtoendPeopleDetection2015}. The model (\ref{Methods:Model}) predicts a fixed-size set of $N$ potential objects per timestep. The parameter $N$ is a dimension of the latent array and is chosen to be greater than the cardinality of the largest set of ground-truth objects per frame. The model uses an attention mechanism that allows it to attend to all other elements in the attention map. Therefore, there is no predefined structure, like a grid in one-stage image detectors \cite{}, associating predictions with ground-truth objects. This presents the core challenge of how to score the fixed-size prediction set against the variable-size ground-truth set of objects. The loss function calculation consists of two steps: first, finding an optimal bipartite matching between the elements of the predicted and ground-truth sets, and second, calculating a loss.

Let us denote the set of predictions for a given timestep as $ \hat{Y} = \{\hat{y}_j\}_{j=1}^N $ and the corresponding set of ground-truth objects for that frame as $ Y = \{y_i\}_{i=1}^M $. At each recurrence (i.e., for each frame/timestep processed), the RPerceiver outputs the entire set of $N$ predictions $\hat{Y}$. Each prediction $\hat{y}_j \in \hat{Y}$ consists of a predicted object representation $\hat{o}_j$ and a predicted class label $\hat{c}_j$ (including the possibility of the no-object class $\emptyset$). The nature of $\hat{o}_j$ depends on the specific task:
\begin{itemize}
    \item For the \textbf{bounding box prediction task}, $\hat{o}_j$ is the predicted box $\hat{b}_j = (\hat{b}_x, \hat{b}_y, \hat{b}_w, \hat{b}_h) \in \mathbb{R}^4$, representing center coordinates, height, and width relative to the frame size.
    \item For the \textbf{center point prediction task}, $\hat{o}_j$ is the predicted center point coordinates $\hat{p}_j = (\hat{p}_x, \hat{p}_y) \in \mathbb{R}^2$ relative to the frame size.
\end{itemize}
Similarly, each ground-truth object $y_i \in Y$ consists of the ground-truth object representation $o_i$ (either a box $b_i$ or a point $p_i$) and the ground-truth class label $c_i$.
We define a matching algorithm via an injective function $f: Y \rightarrow \hat{Y}$, where $f(y_i)$ is the candidate hypothesis $\hat{y}_j \in \hat{Y}$ assigned to the ground-truth object $y_i$. Given $f$, we define a loss function on pairs of sets $Y$ and $\hat{Y}$ as \cite{}:
\begin{equation} \label{eq:set_loss}
L(Y, \hat{Y}, f ) = \sum_{i: f(y_i) \text{ is defined}} l_{pos}(y_i, f(y_i)) +  \sum_{j=1}^{N} l_c (\hat{y}_j, y_j^{\text{match}})
\end{equation}
where $l_c$ is the class prediction loss, for which we utilize the Focal Loss \cite{}, and $l_{pos}$ is an object localization loss. The formulation of $l_{pos}$ depends on the specific task (bounding box or center point). Details on each loss term are provided below.

\textbf{Focal Loss.} The Focal Loss is used to address the class imbalance between the foreground objects and the numerous potential background predictions. The class loss component $l_c$ for each prediction $\hat{y}_j$ is calculated using the sigmoid Focal Loss, summed over all foreground classes $k \in \{1, ..., C\}$, where $C$ is the number of object categories (excluding the $\emptyset$ class). The Focal Loss for a single class $k$ and prediction $j$ is defined as:
\begin{equation} \label{eq:focal_loss}
    \text{FL}(p_{jk}) = - \alpha_t (1 - p_{jk})^\gamma \log(p_{jk})
\end{equation}
where $p_{jk} = \sigma(x_{jk})$ is the predicted sigmoid probability for class $k$ from the raw logits $x_{jk}$, and $\alpha_t$ and $p_t$ are defined based on the ground-truth label $y_{jk}$ (which is 1 if the matched ground-truth class is $k$, and 0 otherwise):
\begin{equation*}
    p_t =
    \begin{cases}
        p_{jk} & \text{if } y_{jk} = 1 \\
        1 - p_{jk} & \text{if } y_{jk} = 0
    \end{cases}
\end{equation*}
The term $\gamma \ge 0$ is the focusing parameter, which reduces the relative loss for well-classified examples, thereby putting more focus on hard, misclassified examples. The term $\alpha_t$ is a weighting factor to address class imbalance, typically $\alpha$ for the positive class ($y_{jk}=1$) and $1-\alpha$ for the negative class ($y_{jk}=0$). The total class loss for prediction $j$ is $l_c(\hat{y}_j, y_j^{\text{match}}) = \sum_{k=1}^{C} \text{FL}(p_{jk})$. For unmatched predictions ($\hat{y}_j$ matched to $\emptyset$), $y_{jk}=0$ for all foreground classes $k$.

\textbf{Bounding Box Loss} is a linear combination of the L1 loss and the Generalized IoU (GIoU) loss \cite{}:
\begin{equation}
    \mathcal{L}_{box}(b_i, \hat{b}_j) = \lambda_{L1} \mathcal{L}_{L1}(b_i, \hat{b}_j) + \lambda_{giou} \mathcal{L}_{giou}(b_i, \hat{b}_j)
    \label{eq:box_loss_revised}
\end{equation}
where $y_i = (c_i, b_i)$ and $\hat{y}_j = (\hat{c}_j, \hat{b}_j)$, and:
\begin{itemize}
    \item $ \mathcal{L}_{L1}(b_i, \hat{b}_j) = \|b_i - \hat{b}_j\|_1 $ is the L1 norm, weighted by $ \lambda_{L1} $ (e.g., \texttt{weight\_dict['loss\_bbox'] * bbox\_loss\_coef}).
    \item $ \mathcal{L}_{giou}(b_i, \hat{b}_j) = 1 - \text{GIoU}(\text{box\_ops.box\_cxcywh\_to\_xyxy}(b_i), \text{box\_ops.box\_cxcywh\_to\_xyxy}(\hat{b}_j)) $, weighted by $ \lambda_{giou} $ (e.g., \texttt{weight\_dict['loss\_giou'] * giou\_loss\_coef}). GIoU is defined in \cite{Rezatofighi2019GeneralizedIO}. % Added GIoU citation
\end{itemize}
Both the $ \mathcal{L}_{L1} $ and $ \mathcal{L}_{giou} $ components are normalized by the total number of actual ground-truth boxes ($M$) across the batch (denoted as \texttt{num\_boxes}). Thus, for matched pairs in the bounding box task, $l_{pos}(y_i, \hat{y}_j) = \mathcal{L}_{box}(b_i, \hat{b}_j)$.

\textbf{Center Point Loss.} For the center point prediction task, the localization loss $l_{pos}(y_i, \hat{y}_j)$ applies \textit{only} to matched pairs $(i, j)$ where $f(y_i) = \hat{y}_j$. we use the L1 distance between the ground-truth center point $p_i$ and the predicted center point $\hat{p}_j$:
\begin{equation} \label{eq:point_loss}
    \mathcal{L}_{point}(p_i, \hat{p}_j) = \|p_i - \hat{p}_j\|_1
\end{equation}
where $y_i = (c_i, p_i)$ and $\hat{y}_j = (\hat{c}_j, \hat{p}_j)$. The total localization loss term in Equation \ref{eq:set_loss} for this task involves summing $\mathcal{L}_{point}$ over all matched pairs and normalizing by the total number of ground-truth objects ($M$) in the batch (denoted as \texttt{num\_objects}). Thus, for matched pairs in the center point task, $l_{pos}(y_i, \hat{y}_j) = \mathcal{L}_{point}(p_i, \hat{p}_j)$.

\textbf{Matching Function ($ f $):} This optimal assignment is computed efficiently using the Hungarian algorithm \cite{Kuhn1955TheHM, Carion2020EndToEndOD, Stewart2015EndtoendPD}. The pairwise matching cost $\mathcal{L}_{match}(y_i, \hat{y}_j)$ used within the Hungarian algorithm depends on the task. For the \textbf{bounding box task}, it is defined as \cite{Carion2020EndToEndOD}:
\begin{equation} \label{eq:matching_cost_bbox}
\mathcal{L}_{match}^{\text{bbox}}(y_i, \hat{y}_j) = - \mathbb{I}\{c_i \neq \emptyset\} \hat{p}_{j}(c_i) + \mathbb{I}\{c_i \neq \emptyset\} \mathcal{L}_{box}(b_i, \hat{b}_{j})
\end{equation}
where $\hat{p}_j(c_i)$ is the predicted probability of the ground-truth class $c_i$ for prediction $j$. For the \textbf{center point task}, the cost replaces $\mathcal{L}_{box}$ with the appropriate point localization cost $\mathcal{L}_{point}$:
\begin{equation} \label{eq:matching_cost_point}
\mathcal{L}_{match}^{\text{point}}(y_i, \hat{y}_j) = - \mathbb{I}\{c_i \neq \emptyset\} \hat{p}_{j}(c_i) + \mathbb{I}\{c_i \neq \emptyset\} \mathcal{L}_{point}(p_i, \hat{p}_{j})
\end{equation}
The optimal assignment $\hat{\sigma}$ minimizes the total matching cost over possible permutations $\sigma$ of the predictions using the task-specific cost:
\begin{equation} \label{eq:hungarian_argmin}
\hat{\sigma} = \arg\min_{\sigma \in \mathfrak{S}_N} \sum_{i=1}^{M} \mathcal{L}_{match}^{\text{task}}(y_i, \hat{y}_{\sigma(i)})
\end{equation}
where $\mathfrak{S}_N$ is the set of permutations of $\{1, ..., N\}$. The function $f$ used in Equation \ref{eq:set_loss} is then defined by this optimal assignment $\hat{\sigma}$, such that $f(y_i) = \hat{y}_{\hat{\sigma}(i)}$ for matched ground-truth objects $y_i$. The overall loss computed using this optimal matching is referred to as the Hungarian loss, $\mathcal{L}_{Hungarian}(Y, \hat{Y}) = L(Y, \hat{Y}, f_{\hat{\sigma}})$.

\subsection{Dropout and Shuffle} \label{Methods:Training:DropoutAndShuffle}

We implemented two training procedures: \texttt{shuffle} and \texttt{dropout}. Additionally, we implemented a combination of the two. A model trained without applying these training procedures is considered the baseline.

\begin{description}
    \item[\texttt{shuffle}] In this procedure, the sensor inputs are randomly permuted within each time step. Consequently, the model receives inputs from the sensors in a random order for that specific time step. This shuffling only occurs for sensor inputs within the same time step. This training procedure is only applicable to a multi-view setup.

    \item[\texttt{dropout}] This procedure simulates scenarios where sensor information is missing. To achieve this, we train the model by intermittently dropping sensor inputs (input dropout). We keep the first half of the input sequence intact (no dropout), allowing the model to accumulate features in its hidden state. The second half of the sequence may undergo frame dropout depending on the dropout probability. During training, we gradually increase the probability of an information dropout from 10\% up to 86.6\%.
\end{description}


% TODO add schema of shuffle and dropout

% TASK DEFINITION
