\section{Background}  \label{Background}



\subsection{Autonomous Driving Systems} \label{Background:AutonomousDrivingSystems}

% thesis: Autonomous Driving Systems have high safety requirements.

Autonomous Driving Systems (ADS), defined by the Society of Automotive Engineers (SAE) as the hardware and software that are collectively capable of performing the entire dynamic driving task (DDT) on a sustained basis, should have a high safety requirements.
This term is used to describe the three most advanced Levels 3 (Conditional), 4 (High), and 5 (Full) driving automation system \cite{sae:j3016:2021apr}. Additionally, ADS must be capable of operating under diverse operational conditions, defined as the operational design domain (ODD) \cite{sae:j3016:2021apr}. The ODD includes, but is not limited to, environmental, geographical, and time-of-day conditions.
While operating under these diverse conditions, ADS can experience DDT performance-relevant system failures that prevent it from reliably performing the DDT on a sustained basis.
In such cases, the ADS must issue a request to intervene, either for the user to perform the DDT or for the system to achieve a minimal risk condition.
At Level 3, a DDT fallback-ready user is expected to take over the DDT when a DDT performance-relevant system failure occurs or when the ADS is about to leave its operational design domain (ODD). This user must be receptive and able to resume DDT performance when alerted to do so.
For example, a Level 3 ADS experiences a DDT performance-relevant system failure in one of its radar sensors, which prevents it from reliably detecting objects in the vehicle's pathway. The ADS responds by issuing a request to intervene to the DDT fallback-ready user. The ADS continues to perform the DDT, while reducing vehicle speed, for several seconds to allow time for the DDT fallback-ready user to resume operation of the vehicle in an orderly manner. It is important to highlight that a key expectation for Level 3 ADS is the capability of continuing to perform the DDT for at least several seconds after issuing the fallback-ready user with a request to intervene.
For Levels 4 and 5 ADS, if a driving automation system can perform the entire DDT and DDT fallback either within a prescribed ODD (Level 4) or in all driver-manageable on-road operating situations (Level 5), then any users present in the vehicle while the ADS is engaged are passengers. For example, if a Level 4 ADS experiences a DDT performance-relevant system failure in one of its computing modules, it transitions to DDT fallback by engaging a redundant computing module(s) to achieve a minimal risk condition.
The high safety requirements defined by SAE imply significant robustness expectations for ADS. This means that an ADS should be capable of, first, performing the DDT for a few seconds following a performance-relevant system failure until DDT fallback, and second, in the case of Levels 4 and 5, it must be able to reach a minimal risk condition despite such a system failure \cite{sae:j3016:2021apr}.

% goal: explain relevance between sensors and video object detection task

ADS should be capable of perceiving data from various sources. The DDT is defined as all real-time operational and tactical functions required to operate a vehicle in on-road traffic. Object and Event Detection and Response (OEDR) refers to the subtasks of the DDT that include monitoring the driving environment (detecting, recognizing, and classifying objects and events, and preparing to respond as needed) and executing an appropriate response to such objects and events (i.e., actions required to complete the DDT and/or DDT fallback) \cite{sae:j3016:2021apr}. Performing the OEDR is necessary to complete the DDT and DDT fallback, and the capability of a Driving Automation System to handle OEDR allows it to be classified as Level 3 automation or higher. OEDR is also known as perception and a variety of computer vision tasks fall under this category, such as object detection, semantic segmentation, 3D object detection, and others.

OEDR tasks rely on exteroceptive sensors for perception hardware, which can be susceptible to system failures that can compromise overall system safety. ADS system architecture is predominantly realized with two approaches: modular or end-to-end \cite{yurtseverSurveyAutonomousDriving2020}. In both approaches, pipelines start with feeding raw sensor inputs to downstream system, either localization and object detection modules or an end-to-end model see the Figure \ref{fig:figure_background_ads_system_architecture}. Exteroceptive sensors, mainly used for commonly employed modalities for perceiving the environment, include cameras, LiDAR (light detection and ranging), radar, and ultrasonic sensors. These sensors vary in technologies and may exhibit various failures \cite{matosSurveySensorFailures2024}. The scope of this thesis will specifically investigate two critical failure modes: (1) non-deterministic sensor input availability, where the system receives inconsistent or temporally varying subsets of data from its sensors, and (2) the complete failure of a sensor component, where it provides no signal. Temporal Calibration, a mitigation strategy, establishes the synchronization rate of multiple sensor data streams \cite{matosSurveySensorFailures2024}. While Temporal Calibration aims to mitigate the first failure mode of non-deterministic sensor input, it cannot rule it out completely. This limitation stems from the asynchronous publish-subscribe communication model inherent in common ADS middleware frameworks like the Robot Operating System (ROS). In such systems, message delivery times for sensor data are not strictly guaranteed and can be influenced by variable factors including network latency, system processing load, and underlying operating system scheduling priorities. Consequently, even when employing ROS tools designed for time synchronization, these inherent timing uncertainties and potential for message delays or drops mean that perfect, deterministic, real-time alignment of all sensor data streams cannot always be achieved, leaving residual non-determinism in sensor input availability \cite{parkRealTimeCharacteristicsROS2020}. The complete sensor failure has no mitigation strategy except sensor redundancy \cite{matosSurveySensorFailures2024}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_background_ads_system_architecture.png}
    \caption{Information flow diagrams of: (a) a generic modular system, and (b) an end-to-end driving system \cite{yurtseverSurveyAutonomousDriving2020}.}
    \label{fig:figure_background_ads_system_architecture}
\end{figure}

\subsection{Video Object Detection} \label{Background:VideoObjectDetection}

% Object detection definition. Deep learning pushed performance of single image object detection.

%TODO: add more exampleas of the progress in CV (from Object Detection from Video Tubelets with Convolutional Neural Networks)
Object detection is a foundational challenge in computer vision and has been a subject of research for several decades \cite{fischlerRepresentationMatchingPictorial1973}. The goal of the object detection task is to find objects of a given description in images and videos.
Advancements in deep learning techniques for feature representation learning \cite{hintonReducingDimensionalityData2006, lecunDeepLearning2015}, in conjunction with the significant development and application of deep Convolutional Neural Networks (CNN), have driven remarkable progress in various computer vision tasks, such as image classification \cite{krizhevskyImageNetClassificationDeep2012}, object detection \cite{girshickRichFeatureHierarchies2014a}.
Extending detection capabilities from static images to video sequences introduces the task of video object detection, which involves not only localizing objects within each frame but also leveraging temporal information across frames for improved accuracy and consistency.
The introduction of specific challenges, such as the video object detection track in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) \cite{russakovskyImageNetLargeScale2015}, provided benchmark datasets and standardized evaluation protocols, significantly accelerating research and development in the video object detection domain.

% State-of-the-Art
% Still-image detector
Due to the inherent similarity between detecting objects in single images and in video frames, the most straightforward approach to video object detection task is to apply a single-image object detector independently to each frame. 
This method, often referred to as frame-by-frame detection, treats each frame as independent image. %TODO: add citation where it's refered as f-to-f
However, such an approach ignores the rich temporal and contextual information available across consecutive video frames. Neglecting this temporal dimension often leads to suboptimal performance, characterized by issues like inconsistent bounding box predictions across frames, flickering detections, and reduced robustness to challenges specific to video, such as motion blur, occlusion, morphological diversity, and illumination variations within the video \cite{jiaoNewGenerationDeep2022}. 
Consequently, while applying static detectors frame-by-frame serves as a simple baseline, it is generally not considered an effective or optimal solution for the complexities of video object detection. %TODO: Add citation with examples

% For example, if an object is detected in neighboring frames but not in the current frame, we can recover the missing object in the current frame by applying temporal coherence.
% Another example is that mistakenly-labeled objects can be corrected by checking the semantic labels across the frames.

Video object detection algorithms can be broadly classified based on their architectural solution to the video object detection task. For this thesis, the classification proposed by the survey \cite{jiaoNewGenerationDeep2022} is adopted. This survey categorizes these algorithms into four main types: those based on image detection (postprocessing methods), those utilizing motion information (introducing additional models), those employing feature filtering, and other effective network structures.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_background_vod_classification.png}
    \caption{Classification of video object detection problems and solutions \cite{jiaoNewGenerationDeep2022}.}
    \label{fig:figure_background_vod_classification}
\end{figure}

% Postprocessing (not e2e)
% T-CNN: ImageNet VID 2015 73 - 77 mAP
One strategy involves applying a postprocessing step to the outputs of a still-image detector to enhance temporal consistency \cite{hanSeqNMSVideoObject2016, kangTCNNTubeletsConvolutional2018, kangObjectDetectionVideo2016}. This often utilizes detections from adjacent frames to refine the results for the current frame. Common postprocessing techniques include Sequence Non-Maximum Suppression (Seq-NMS), which links high-scoring detection boxes across frames into sequences \cite{hanSeqNMSVideoObject2016}, or leveraging optical flow to propagate detection scores \cite{kangTCNNTubeletsConvolutional2018, kangObjectDetectionVideo2016}. While these methods can improve upon static image detectors, exploiting temporal information solely during postprocessing is considered suboptimal, as crucial temporal and motion cues are disregarded during the primary detector training phase. Therefore, such algorithms have difficulty overcoming consecutive failures of the static image detector when the object of interest experiences long-term occlusions or significant appearance changes.

% motion information

To address the limitation of postprocessing, another category of methods utilizing motion information by introducing additional models to integrate motion and temporal information directly into the model training, often creating end-to-end solutions. This category is subdivided into models based on optical flow, contextual information, and trajectory information. For the purpose of this thesis, a subcategory of contextual information is reviewed, which utilizes Recurrent Neural Networks (RNN) and their variants. The power of RNNs in long-range temporal representation has become a valuable tool for the video object detection task \cite{Lu_2017_ICCV, xiaoVideoObjectDetection2018, liuMobileVideoObject2018}, enabling the design of end-to-end networks. One of the challenges posed by such methods is how to associate objects within the RNN structure across multiple frames.

One notable early attempt to tackle this association problem is the Association LSTM framework \cite{Lu_2017_ICCV}. The framework consists of an SSD image detector \cite{liuSSDSingleShot2016} and a variant of the RNN model, the Long Short-Term Memory (LSTM) network \cite{6795963}. SSD detects frame-wise, image-based object detection results (bounding box, score, and object feature) which are stacked and fed into the LSTM. The association LSTM not only regresses and classifies directly on object locations and categories but also associates features to represent each output object. By minimizing the matching error between these features, the network learns how to associate objects in two consecutive frames. Additionally, the method works in an online manner, which is important for real-world applications. The authors acknowledge that a weakness of these frameworks is that the LSTM module is a post-hoc addition, since performance is limited by the quality of the initial SSD detections. Missed or poorly localized detections by the SSD are difficult for the LSTM to recover. Furthermore, the SSD parameters are not updated during training.

% T-CNN with LSTM
% TUBELETS: ImageNet VID 2015 ~75ish mAP

% % STMN: ImageNet VID 2015 80.5 mAP
Building upon the idea of modeling temporal dependencies but aiming for deeper integration and leveraging spatial information more effectively, the Spatial-Temporal Memory Network (STMN) was proposed \cite{xiaoVideoObjectDetection2018}. Unlike the Association LSTM which operates on vector-form features within a standard LSTM, STMN introduces a Spatial-Temporal Memory Module (STMM). This module utilizes a modified bidirectional convolutional Gated Recurrent Unit (ConvGRU) \cite{ballasDelvingDeeperConvolutional2016}, allowing it to process and retain information in a spatially structured manner directly from convolutional feature maps generated by the detector backbone. By preserving spatial locality within the recurrent computation, STMM can better capture appearance changes and motion patterns. Experimental results indicated that the effectiveness of such convolutional memory modules increases with the length of the input sequence, as more relevant long-range context can be accumulated, leading to improved detection accuracy, particularly for challenging scenarios involving occlusion or significant appearance variations \cite{xiaoVideoObjectDetection2018}.

The principle of using convolutional recurrent units to efficiently propagate spatio-temporal information proved beneficial not only for accuracy but also for enabling real-time video object detection on resource-constrained platforms. In this work \cite{liuMobileVideoObject2018}, an architecture is introduced that augments a single-image object detector (SSD with a pruned MobileNet base) by interweaving efficient convolutional LSTM (ConvLSTM) \cite{} layers, specifically "Bottleneck-LSTM" layers, to refine and propagate feature maps across frames. Despite its more complex architecture, an array of modifications is proposed that allow this model to be faster and more lightweight than mobile-focused single-frame models.

% Feature filtering

Another category of video object detection methods employs feature filtering techniques to enhance performance by selectively focusing on relevant spatiotemporal information while suppressing redundant or irrelevant data. This approach draws inspiration from the human visual system, which can rapidly identify salient regions within a scene, thereby optimizing cognitive resources for efficient analysis \cite{}. Similarly, feature filtering mechanisms in neural networks aim to prioritize critical features and reduce unnecessary computations, leading to improvements in both accuracy and efficiency \cite{jiaoNewGenerationDeep2022}. These methods can be broadly subdivided based on the specific filtering mechanism employed, notably attention mechanisms \cite{bahdanauNeuralMachineTranslation2016a, vaswaniAttentionAllYou2023} and deformable convolutions \cite{}.

Attention mechanisms allow networks to dynamically weigh the importance of different features or spatial locations across video frames. This selective focus enables the propagation of crucial information, particularly for objects undergoing significant appearance changes or movements, potentially offering advantages over methods relying solely on adjacent frame correlations or optical flow, which can struggle with large displacements and add significant model complexity \cite{jiaoNewGenerationDeep2022, guoProgressiveSparseLocal2019}.

One prominent example is the Progressive Sparse Local Attention (PSLA) framework \cite{guoProgressiveSparseLocal2019}. Addressing limitations associated with optical flow, such as increased model size and difficulty handling large displacements or high-level features, PSLA provides an alternative mechanism for establishing feature correspondence and propagating information between frames \cite{guoProgressiveSparseLocal2019}. Instead of calculating pixel-level flow, PSLA operates directly on feature maps. While similar to STMN \cite{xiaoVideoObjectDetection2018} which also uses local correlation for alignment, PSLA differs by utilizing a sparse neighborhood and softmax normalization for better spatial correspondence, aiming to improve both speed and accuracy \cite{guoProgressiveSparseLocal2019}. It uses a special attention approach called progressive sparse stride, which pays more attention to nearby features (for small movements) and less attention to features farther away (for larger movements). By calculating weighted correspondences based on feature similarity within this sparse local neighborhood, PSLA aligns and aggregates features across time, enabling temporal feature updating and enhancement without relying on an explicit optical flow model. This approach aims to achieve a better balance between accuracy, speed, and model size compared to traditional flow-based methods \cite{guoProgressiveSparseLocal2019}. Nevertheless, managing the complexity of attention calculations remains a factor.

More recently, Transformer architectures \cite{vaswaniAttentionAllYou2017} have demonstrated significant potential in computer vision tasks, including object detection \cite{carionEndToEndObjectDetection2020, zhuDeformableDETRDeformable2021}. Models like DETR (DEtection TRansformer) \cite{carionEndToEndObjectDetection2020, zhuDeformableDETRDeformable2021} and its derivatives aim to simplify the traditional detection pipeline by reformulating object detection as a set prediction problem, eliminating the need for hand-designed components like NMS or anchor generation found in many CNN-based detectors. While newer to video object detection compared to RNNs, Transformers offer a different paradigm for modeling long-range dependencies and object relationships within sequences \cite{wangEndtoEndVideoObject2021, shvetsTrackingObjectsAs2021}.

\subsection{General Purpose Perceiver Model} \label{Background:Perceiver}

Most architectures used by AI systems today are specialized. For instance, models presented in \ref{Background:VideoObjectDetection} built for the video object detection task might excel at processing 2D video frames, but they are hardly ideal for other data types, such as the LiDAR point clouds or radar output used in ADS. Handling multiple data modalities, like the sounds and images that make up videos, presents even greater complexity and usually involves complex, hand-tuned systems built from many different parts, even for simple tasks. Real-world problems, such as building an ADS, possess these complexities, so there is a desperate need to build a simple yet effective, more general, and versatile architecture that can handle all types of data.

Perceiver \cite{jaeglePerceiverGeneralPerception2021} was introduced as a general-purpose architecture that is capable of processing different data types such as images, point clouds, audio, video, and, what is important, combinations of those to fuse together. The Perceiver builds upon the Transformer \cite{vaswaniAttentionAllYou2023}, an architecture that uses an operation called "Attention" to map inputs into outputs \cite{bahdanauNeuralMachineTranslation2016a}. While attention is simple and widely applicable, the way Transformers utilize attention can become memory expensive as the number of inputs grows. Consequently, Transformers perform well with inputs containing at most a few thousand elements, but common data forms like images and videos can easily comprise millions of elements. This fact poses a challenge to the generalist architecture: scaling the Transformer's attention operation to very large inputs without introducing domain-specific assumptions. The Perceiver addresses this by using attention to first encode the inputs into a small latent array. This latent array can then be processed further at a cost independent of the input's size, allowing the Perceiver's memory and computational needs to scale gracefully as the input size grows, even for particularly deep models. This "graceful growth" enables the Perceiver to achieve an unprecedented level of generality - it is competitive with domain-specific models on benchmarks based on images, 3D point clouds, and combined audio and images \cite{jaeglePerceiverGeneralPerception2021}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/figure_background_perceiver_architecture.png}
    \caption{The Perceiver is an architecture based on attentional principles that scales to high-dimensional inputs such as images, videos, audio, point-clouds, and multimodal combinations without making domain-specific assumptions. The Perceiver uses a cross-attention module to project an high-dimensional input byte array to a fixed-dimensional latent bottleneck (the number of input indices M is much larger than the number of latent indices $N$) before processing it using a deep stack of Transformer-style self-attention blocks in the latent space. The Perceiver iteratively attends to the input byte array by alternating cross-attention and latent self-attention blocks \cite{jaeglePerceiverGeneralPerception2021}.}
    \label{fig:figure_background_perceiver_architecture}
\end{figure}

The Perceiver architecture consists of two primary components: (i) a cross-attention module that maps an input byte array (e.g. a pixel array) and a latent array to an updated latent array, and (ii) a Transformer tower that maps this latent array to another latent array \cite{jaeglePerceiverGeneralPerception2021}. The Perceiver architecture is illustrated in Figure \ref{fig:figure_background_perceiver_architecture}. The model can be conceptualized as a recurrent neural network (RNN) unrolled in depth with the same input, rather than unrolled in time. A central challenge addressed by this architecture is the scaling of attention mechanisms to accommodate very large and diverse inputs. The Perceiver mitigates the quadratic complexity bottleneck inherent in standard Transformers by employing its cross-attention module. This module introduces an asymmetry into the attention operation when applied directly to the inputs.

Specifically, for a query $Q \in \mathbb{R}^{M \times D}$, key $K \in \mathbb{R}^{M \times C}$, and value $V \in \mathbb{R}^{M \times C}$ (where $C$ and $D$ represent channel dimensions), the computational complexity of the standard $QKV$ attention operation---formulated as $\text{softmax}(QK^T)V$---is $\mathcal{O}(M^2)$. This is due to matrix multiplications involving the large input index dimension $M$. The authors of Perceiver introduced an asymmetry: while $K$ and $V$ are projections of the input byte array (with $M$ elements), $Q$ is a projection of a learned latent array characterized by a much smaller index dimension $N \ll M$. The dimension $N$ of this latent array is a hyperparameter. Consequently, the resulting cross-attention operation exhibits a complexity of $\mathcal{O}(MN)$. It is important to note that within the Perceiver's cross-attention, linear projection layers are applied to generate $Q$, $K$, and $V$ such that they share a common channel dimension before the attention calculation.

This design enables Perceiver-based architectures to leverage significantly deeper Transformers compared to efficient Transformer variants that use linear complexity layers, without depending on domain-specific assumptions. Using $L$ to represent the number of layers in the Transformer tower, the complexity of a standard Transformer operating directly on $M$ input elements (bytes) can be represented as $\mathcal{O}(LM^2)$, whereas the complexity of Perceiver's latent Transformer (processing the $N$-dimensional latent array) is $\mathcal{O}(LN^2)$. Given that $N \ll M$, this substantially reduces the computational cost per layer. The overall architecture's complexity thus becomes $\mathcal{O}(MN + LN^2)$ for a latent Transformer with $L$ layers. This decoupling of the input size from the network depth is crucial, as it permits the addition of Transformer layers at a cost that is independent of the input size.

However, because the original Perceiver produced only one output per input, it was not as versatile as researchers required. Our proposed architecture aims to address this limitation by modifying the Perceiver architecture to make it akin to a recurrent unit processing input over time.


