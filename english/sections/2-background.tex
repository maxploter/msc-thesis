\section{Background}  \label{Background}



\subsection{Autonomous Driving Systems} \label{Background:ADS}

% thesis: Autonomous Driving Systems have high safety requirements.

The Society of Automotive Engineers (SAE) defines Autonomous Driving Systems (ADS) as the hardware and software that are collectively capable of performing the entire dynamic driving task (DDT) on a sustained basis, regardless of whether it is limited to a specific operational design domain (ODD). The DDT encompasses all real-time operational and tactical functions required for driving, including lateral and longitudinal motion control, and Object and Event Detection and Response (OEDR).

\cite{sae:j3016:2021apr}



ADS term is used to describe the last three and the most advanced SAE Levels 3 (Conditional), 4 (High), and 5 (Full) driving automation. ADS operate under high safety requirements, therefore crash mitigation and avoidance capability is part of ADS functionality.

An Automated Driving System (ADS) refers to the hardware and software
collectively capable of performing the entire Dynamic Driving Task (DDT)
on a sustained basis, applicable to SAE Levels 3 (Conditional), 4
(High), and 5 (Full) driving automation{ \textsuperscript{} }.

Unlike
lower levels of automation which merely assist a human driver, an ADS
takes full responsibility for operating the vehicle within its specified
Operational Design Domain (ODD), if applicable{ \textsuperscript{} }.

This transfer of complete, real-time operational and tactical control {
\textsuperscript{} } inherently necessitates that Autonomous Driving
Systems operate under high safety requirements. The complexity involved
in perceiving the environment, making safety-critical decisions, and
executing maneuvers without human oversight demands rigorous design,
validation, and operational safeguards, particularly concerning the
system\textquotesingle s ability to manage unforeseen events and
internal malfunctions safely{ \textsuperscript{} }. { ~ }

A critical aspect supporting the high safety requirements for ADS
revolves around \textbf{Failure Mitigation and Safe States}. ADS are
complex integrations of hardware, software, and sensors, components of
which are inevitably subject to DDT performance-relevant system failures
-- malfunctions preventing the reliable performance of the driving task
{ \textsuperscript{} } (P1). To uphold safety during such occurrences,
ADS must incorporate predefined strategies for DDT fallback{
\textsuperscript{} }. This involves the system responding appropriately
to a failure or ODD exit, which for Level 4 and 5 systems means the ADS
must automatically achieve a Minimal Risk Condition (MRC) -- a stable,
stopped state designed to reduce crash risk{ \textsuperscript{} }. For
Level 3 systems, fallback may involve issuing a timely request to
intervene to a fallback-ready user, expecting them to resume the DDT or
achieve an MRC { \textsuperscript{} } (P2). During such a transition in
Level 3, the ADS is expected to continue performing the DDT safely for a
period to allow the user time to respond appropriately {
\textsuperscript{} } (P3). Consequently, robust failure detection
capabilities coupled with comprehensive mitigation strategies are
essential for ADS to reliably transition the vehicle to an appropriate
safe state -- whether that is under user control following a fallback
request or an automatically achieved MRC -- upon system malfunction or
performance degradation { \textsuperscript{} } (C1). { }


There's an expectation of 

State-of-the-art ADS employ a wide variety of sensors, often with redundancy, to ensure safety. These sensors can be categorized based on their purpose. In this thesis, we focus on exteroceptive sensors, which are primarily used for perceiving the surrounding environment to accomplish tasks necessary for the ADS.





% goal: explain relevance between ADS and video object detection task

Autonomous vehicle are complex on multi-sensor perception, integrating data from various sources such as LiDAR, cameras, and radar to create a comprehensive understanding of the environment.

Definition of autonomous driving systems. Explain architecture.

Type of sensors in ADS.

\subsection{Sensors} \label{Background:Sensors}

% goal: explain relevance between sensors and video object detection task


\subsection{Video Object Detection} \label{Background:VideoObjectDetection}

% Object detection definition. Deep learning pushed performance of single image object detection.

Object detection is a foundational challenge in computer vision and has been a subject of research for several decades \cite{fischlerRepresentationMatchingPictorial1973}. The goal of the object detection task is to find object(s) of given description in images and videos. Advancements in deep learning techniques to learn feature representations \cite{hintonReducingDimensionalityData2006, lecunDeepLearning2015} have made it possible to achive remarkable progress in single-image object detectors \cite{girshickRichFeatureHierarchies2014a}.

% State-of-the-Art
% Still-image detector

Due to similarity beetwen object detection on image and video, the simplest approach to tackle a video object detection is to use a single-image object detector, so treat each video frame as an independent image. However, this approach is naive because it completely ignores temporal and contextual information of the video. A still image detector is not able to consistently predict objects acrsso all video frames due to numerouse artefactos and biases in the video (e.g., motion blur, occlusion, unusual position).

% Postprocessing (not e2e)

%One intuition to improve temporal consistency is to propagate detection results to neighbor frames to reduce sudden changes of detection results.

%In this work we propose a simple extension of single image object detection to help overcome these difficulties.

Another simple approach which improves temporal consistency of video object detections involves a postprocessing step after still image detector \cite{hanSeqNMSVideoObject2016, kangTCNNTubeletsConvolutional2018, kangObjectDetectionVideo2016}. Detections from adjecent frames are used to improve results of the current frame. At postprocessing step researches apply technich like non-maximum suppression (NMS) \cite{hanSeqNMSVideoObject2016} or optical flow \cite{kangTCNNTubeletsConvolutional2018, kangObjectDetectionVideo2016}. Although these methods show improvement over a stil image detector aproach, exploiting temporal information as postprocessing is sub-optimal since temporal and motion information are ignored during detector training.

% TODO: paper Multi-Class Multi-Object Tracking using  Changing Point Detection

% VIDEO DETECTION WITH ADDITIONAL MODELS

Another approach suggests to introduce motion and temporal information during training to form an end-to-end solution. In \cite{Lu_2017_ICCV} proposed to use a LSTM \cite{6795963} to refine the detection results together with association features which representation of the detected objects. This model is end-to-end and can be trained jointly with the object detector. This algorithm rely on objects association between adjacent frames therefore it is limited to short-term motion information and cannot handle long occlusions or unusual appearance of for a long time which is the same problem with algorithms based on postprocessing. Another work proposes a recurrent computation unit called spatial-temporal memory module (STMM) \cite{xiaoVideoObjectDetection2018} which uses a ConvGRU \cite{ballasDelvingDeeperConvolutional2016} to preserve a spacio temporal structure and show suggests the effectiveness of our memory: the longer the sequence, the more longer-range useful information is stored in the memory, which leads to better detection performance.

% Transformers

Transformers [32], [33], [34], [35], [36] have shown promising potential in computer vision. Especially, DETR [33], [34] simplifies the detection pipeline by modeling the object queries and achieving comparative performance with highly optimized CNN-based detectors.

Thus, it is in desperate need to build a simple yet effective VOD framework in a fully end-to-end manner.

\subsection{General purpose Perceiver model} \label{Background:VideoObjectDetection}