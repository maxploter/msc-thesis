\section{Introduction} \label{Introduction}

% Justification for the choice of topic
% actuality

Autonomous Driving Systems (ADS) hold immense promise, envisioning a future of transportation with enhanced road safety, improved traffic flow, and reduced environmental impact \cite{litmanAutonomousVehicleImplementationb}. Achieving this vision requires ADS to operate reliably under a wide spectrum of diverse and often challenging environmental conditions. This operational requirement necessitates a sophisticated perception stack, heavily reliant on data integrated from multiple sensors. Crucially, the hardware components of this stack are susceptible to system failures, including non-deterministic sensor input availability and complete sensor outages, the two failure modes specifically investigated in this thesis. Such failures in the perception system can have catastrophic consequences, potentially leading to accidents and undermining public trust in autonomous technology \cite{yurtseverSurveyAutonomousDriving2020}. Therefore, ADS necessitate perception models that possess two critical requirements: inherent capability to support the multi-modal hardware stack typical of these systems and robustness against sensor failure.

% Overview Theoretical Background -- background information to contextualize the problem
% review of state of the art solutions
Object detection is a fundamental perception task for Autonomous Driving Systems, a domain that has been significantly advanced by deep learning. Video data, while affording rich spatio-temporal information, introduces distinct challenges such as motion blur, variations in object appearance, and occlusions. The aggregation of temporal information can be accomplished through several methods. Initial approaches frequently concentrated on postprocessing the outputs of static image detectors. These methods employed techniques like non-maximum suppression Seq-NMS~\cite{hanSeqNMSVideoObject2016} to link detections or utilized tracking and optical flow \cite{kangObjectDetectionVideo2016, kangTCNNTubeletsConvolutional2018} to propagate information for temporal consistency. However, these techniques primarily address symptoms because the core detection mechanism does not learn from temporal data, and the methods are not end-to-end. This limitation restricts recovery from occlusions or substantial appearance shifts. To mitigate these shortcomings, subsequent research integrated temporal modeling directly into model architectures. Prominent methodologies encompass Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks and their variants. Examples include Association LSTM~\cite{Lu_2017_ICCV} for inter-frame feature association, and STMN~\cite{xiaoVideoObjectDetection2018} with its Convolutional Gated Recurrent Unit (ConvGRU) for processing spatio-temporal information from feature maps. Alternative strategies focused on feature filtering. These include techniques like attention mechanisms, for instance, PSLA \cite{guoProgressiveSparseLocal2019}, to weigh feature importance across frames and establish correspondence without computationally expensive optical flow. More recently, Transformer-based architectures \cite{vaswaniAttentionAllYou2023} have exerted considerable influence on object detection. Models such as DETR~\cite{carionEndtoEndObjectDetection2020} and Deformable DETR~\cite{zhuDeformableDETRDeformable2021} reformulated the task as a direct set prediction problem, thereby simplifying operational pipelines. These concepts were extended to the video domain through architectures like TransVOD \cite{zhouTransVODEndtoEndVideo2023} and PTSEFormer \cite{wangPTSEFormerProgressiveTemporalSpatial2022}. By interpreting complex spatio-temporal relationships, capturing multi-scale features, and learning long-range dependencies, these Transformer-based models are among the top-performing models on the ImageNet VOD benchmark~\cite{russakovskyImageNetLargeScale2015}.

% Problem statement -- if necessary, it should include the posed hypothesis/hypotheses, research questions, and subject of research

Despite these advancements, a significant gap remains concerning the development of truly generalist perception architectures suitable for the demands of ADS. Many existing state-of-the-art models function as specialists, often built upon architectures with strong inductive biases tailored for specific data types, like 2D video frames processed by CNNs or specific RNN structures. This specialization limits their flexibility and inherent capacity to handle the diverse range of sensor modalities (e.g., LiDAR point clouds, radar returns, thermal imagery alongside video) commonly employed in ADS perception stacks. These models typically lack native mechanisms to process and effectively fuse information from such heterogeneous, high-dimensional inputs without resorting to complex, hand-tuned, multi-component systems. Consequently, building a unified and robust understanding of the driving environment from multi-modal data remains a challenge. This underscores the pressing need for more general-purpose perception architectures, which capable of processing diverse data types without domain-specific assumptions and scale efficiently to large inputs, paving the way for more versatile and robust ADS perception.


% Purpose of the Thesis -- overall aim and objective of the research, "why" of the researchâ€”why are you conducting this study?

% novelty

To address these limitations, this thesis introduces novel recurrent architectures, the Recurrent Perceiver (RPerceiver) and its multi-modal variant, Recurrent Perceiver Multi-Modal (RPerceiverMM). These models are designed as highly generalist recurrent modules, inspired by the Perceiver architecture \cite{jaeglePerceiverGeneralPerception2021}, capable of processing diverse and high-dimensional sequential inputs, including multi-sensor data streams. We present a comprehensive training framework and experimental setup specifically designed to assess not only the object detection performance but also the robustness of these architectures against simulated sensor failure scenarios, demonstrating their potential for safety-critical applications like ADS. Our main contributions are as follows:

\begin{itemize}
    \item We propose a Recurrent Perceiver architecture that, when unrolled in time, can be interpreted as a recurrent neural network (RNN). Furthermore, the model supports multiple sensory inputs.
    \item We introduce a novel benchmark dataset, which we call "detection-moving-mnist-easy", designed to evaluate performance on two distinct tasks: bounding box detection and object center point (keypoint) prediction.    
    \item We propose specific training procedures (e.g., input dropout, shuffling) and evaluation protocols designed to simulate and assess model robustness against potential hardware and software failures, such as complete sensor outages or non-deterministic input availability in single-sensor and multi-sensor setups.
\end{itemize}

This thesis is organized as follows: section \ref{Background} provides background information on autonomous driving system architecture, overviews state of the art video object detection approaches, and introduces the original Perceiver model architecture \cite{jaeglePerceiverGeneralPerception2021}. Section \ref{Methods} presents the novel Recurrent Perceiver architecture, introduces the benchmark used for training and testing, and explains the training procedure. Section \ref{Experiments} explains the experiments and presents the results.

% a short overview of appendices including the content of attached materials
% TODO

% TODO: Question where should i put it?
% This thesis was written using the Overleaf1 text editor. The text was checked with the
% Grammarly2 writing assistant to catch typos and other grammatical errors.