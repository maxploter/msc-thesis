%~~~~~~~~~~~~~~~~~~INTRODUCTION~~~~~~~~~~~~~~~~~~
@article{litmanAutonomousVehicleImplementationb,
  title = {Autonomous {{Vehicle Implementation Predictions}}: {{Implications}} for {{Transport Planning}}},
  author = {Litman, Todd},
  abstract = {This report explores the impacts of autonomous (also called self-driving, driverless or robotic) vehicles, and their implications for transportation planning. It investigates how quickly such vehicles are likely to develop and be deployed based on experience with previous vehicle technologies; their likely benefits and costs; how they will affect travel activity; and their impacts on road, parking and public transit planning. This analysis indicates that Level 5 autonomous vehicles, able to operate without a driver, may be commercially available and legal to use in some jurisdictions by the late 2020s, but will initially have high costs and limited performance. Some benefits, such as independent mobility for affluent non-drivers, may begin in the 2030s but most impacts, including reduced traffic and parking congestion, independent mobility for low-income people (and therefore reduced need for public transit), increased safety, energy conservation and pollution reductions, will only be significant when autonomous vehicles become common and affordable, probably in the 2040s to 2060s, and some benefits may require dedicated autonomous vehicle lanes, which raises social equity concerns.},
  langid = {english},
  keywords = {_st/New},
  file = {/Users/maksimploter/Zotero/storage/3UYQZLD2/Litman - Autonomous Vehicle Implementation Predictions Implications for Transport Planning.pdf}
}



%~~~~~~~~~~~~~~~~~~~BACKGROUND~~~~~~~~~~~~~~~~~~~
@standard{sae:j3016:2021apr,
  organization = {SAE International},
  title        = {Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles},
  number       = {J3016},
  type         = {Surface Vehicle Recommended Practice},
  date         = {2021-04},
  note         = {Revision April 2021, Superseding J3016 JUN2018},
  % Optional: Add version field if preferred over the note
  % version   = {APR2021}, 
  % Optional: Add location if needed by your style
  % location  = {Warrendale, PA},
}

@article{yurtseverSurveyAutonomousDriving2020,
  title = {A {{Survey}} of {{Autonomous Driving}}: {{Common Practices}} and {{Emerging Technologies}}},
  shorttitle = {A {{Survey}} of {{Autonomous Driving}}},
  author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {58443--58469},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2983149},
  url = {https://ieeexplore.ieee.org/document/9046805/?arnumber=9046805},
  urldate = {2025-03-10},
  abstract = {Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.},
  eventtitle = {{{IEEE Access}}},
  keywords = {_r/2_worth_reading,_st/New,_st/Skimmed,Accidents,automation,Automation,Autonomous vehicles,control,intelligent transportation systems,intelligent vehicles,Planning,Robot sensing systems,robotics,Systems architecture,Task analysis,Vehicle dynamics},
  file = {/Users/maksimploter/Zotero/storage/6GL9TIG3/Yurtsever et al. - 2020 - A Survey of Autonomous Driving Common Practices a.pdf;/Users/maksimploter/Zotero/storage/LTSQ7QBW/9046805.html;/Users/maksimploter/Zotero/storage/R6V6KTS8/9046805.html;/Users/maksimploter/Zotero/storage/WAVQYPIK/9046805.html}
}


@article{fischlerRepresentationMatchingPictorial1973,
  title = {The {{Representation}} and {{Matching}} of {{Pictorial Structures}}},
  author = {Fischler, M.A. and Elschlager, R.A.},
  date = {1973-01},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-22},
  number = {1},
  pages = {67--92},
  issn = {1557-9956},
  doi = {10.1109/T-C.1973.223602},
  url = {https://ieeexplore.ieee.org/document/1672195/?arnumber=1672195},
  urldate = {2025-04-03},
  abstract = {The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of "goodness" of matching or detection.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {_st/New,Dynamic programming heuristic optimization picture description picture matching picture processing representation.},
  file = {/Users/maksimploter/Zotero/storage/7D3D26TZ/Fischler and Elschlager - 1973 - The Representation and Matching of Pictorial Structures.pdf;/Users/maksimploter/Zotero/storage/ALZK3YD8/1672195.html}
}

@article{hintonReducingDimensionalityData2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  date = {2006-07-28},
  journaltitle = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1127647},
  url = {https://www.science.org/doi/10.1126/science.1127647},
  urldate = {2025-04-03},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  keywords = {_st/Cited,_st/New,:todo:skim_first},
  file = {/Users/maksimploter/Zotero/storage/BIUQRDPQ/Hinton and Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2024-06-28},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  langid = {english},
  keywords = {_st/Cited,_st/New,:todo:skim_first,Computer science,Mathematics and computing},
  file = {/Users/maksimploter/Zotero/storage/G99AILUJ/LeCun et al. - 2015 - Deep learning.pdf}
}

@inproceedings{girshickRichFeatureHierarchies2014a,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-06},
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  url = {https://ieeexplore.ieee.org/document/6909475},
  urldate = {2025-04-03},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {_st/New,Feature extraction,Object detection,Proposals,Support vector machines,Training,Vectors,Visualization},
  file = {/Users/maksimploter/Zotero/storage/7C92E6PA/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/maksimploter/Zotero/storage/9SRAIGQR/6909475.html}
}

@article{jiaoNewGenerationDeep2022,
  title = {New {{Generation Deep Learning}} for {{Video Object Detection}}: {{A Survey}}},
  shorttitle = {New {{Generation Deep Learning}} for {{Video Object Detection}}},
  author = {Jiao, Licheng and Zhang, Ruohan and Liu, Fang and Yang, Shuyuan and Hou, Biao and Li, Lingling and Tang, Xu},
  date = {2022-08},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {8},
  pages = {3195--3215},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3053249},
  url = {https://ieeexplore.ieee.org/document/9345705/?arnumber=9345705},
  urldate = {2024-09-04},
  abstract = {Video object detection, a basic task in the computer vision field, is rapidly evolving and widely used. In recent years, deep learning methods have rapidly become widespread in the field of video object detection, achieving excellent results compared with those of traditional methods. However, the presence of duplicate information and abundant spatiotemporal information in video data poses a serious challenge to video object detection. Therefore, in recent years, many scholars have investigated deep learning detection algorithms in the context of video data and have achieved remarkable results. Considering the wide range of applications, a comprehensive review of the research related to video object detection is both a necessary and challenging task. This survey attempts to link and systematize the latest cutting-edge research on video object detection with the goal of classifying and analyzing video detection algorithms based on specific representative models. The differences and connections between video object detection and similar tasks are systematically demonstrated, and the evaluation metrics and video detection performance of nearly 40 models on two data sets are presented. Finally, the various applications and challenges facing video object detection are discussed.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Convolution,Deep learning,Detection algorithms,Feature extraction,Learning,Learning systems,neural networks,object detection,Object detection,pipeline processing,Survey,Task analysis,video signal processing},
  file = {/Users/maksimploter/Zotero/storage/PZN6AJZ2/Jiao et al. - 2022 - New Generation Deep Learning for Video Object Dete.pdf;/Users/maksimploter/Zotero/storage/TBVS8TBF/Jiao et al. - 2022 - New Generation Deep Learning for Video Object Detection A Survey.pdf;/Users/maksimploter/Zotero/storage/73S44XGH/9345705.html}
}


@online{hanSeqNMSVideoObject2016,
  title = {Seq-{{NMS}} for {{Video Object Detection}}},
  author = {Han, Wei and Khorrami, Pooya and Paine, Tom Le and Ramachandran, Prajit and Babaeizadeh, Mohammad and Shi, Honghui and Li, Jianan and Yan, Shuicheng and Huang, Thomas S.},
  date = {2016-08-22},
  eprint = {1602.08465},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.08465},
  url = {http://arxiv.org/abs/1602.08465},
  urldate = {2025-03-27},
  abstract = {Video object detection is challenging because objects that are easily detected in one frame may be difficult to detect in another frame within the same clip. Recently, there have been major advances for doing object detection in a single image. These methods typically contain three phases: (i) object proposal generation (ii) object classification and (iii) post-processing. We propose a modification of the post-processing phase that uses high-scoring object detections from nearby frames to boost scores of weaker detections within the same clip. We show that our method obtains superior results to state-of-the-art single image object detection techniques. Our method placed 3rd in the video object detection (VID) task of the ImageNet Large Scale Visual Recognition Challenge 2015 (ILSVRC2015).},
  pubstate = {prepublished},
  keywords = {_st/Cited,Computer Science - Computer Vision and Pattern Recognition,refining detections,video object detection},
  file = {/Users/maksimploter/Zotero/storage/BHMBB4WF/Han et al. - 2016 - Seq-NMS for Video Object Detection.pdf;/Users/maksimploter/Zotero/storage/8HVHZJ8A/1602.html}
}

@article{kangTCNNTubeletsConvolutional2018,
  title = {T-{{CNN}}: {{Tubelets With Convolutional Neural Networks}} for {{Object Detection From Videos}}},
  shorttitle = {T-{{CNN}}},
  author = {Kang, Kai and Li, Hongsheng and Yan, Junjie and Zeng, Xingyu and Yang, Bin and Xiao, Tong and Zhang, Cong and Wang, Zhe and Wang, Ruohui and Wang, Xiaogang and Ouyang, Wanli},
  date = {2018-10},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {10},
  pages = {2896--2907},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2017.2736553},
  url = {https://ieeexplore.ieee.org/document/8003302},
  urldate = {2025-03-27},
  abstract = {The state-of-the-art performance for object detection has been significantly improved over the past two years. Besides the introduction of powerful deep neural networks, such as GoogleNet and VGG, novel object detection frameworks, such as R-CNN and its successors, Fast R-CNN, and Faster R-CNN, play an essential role in improving the state of the art. Despite their effectiveness on still images, those frameworks are not specifically designed for object detection from videos. Temporal and contextual information of videos are not fully investigated and utilized. In this paper, we propose a deep learning framework that incorporates temporal and contextual information from tubelets obtained in videos, which dramatically improves the baseline performance of existing still-image detection frameworks when they are applied to videos. It is called T-CNN, i.e., tubelets with convolutional neueral networks. The proposed framework won newly introduced an object-detection-from-video task with provided data in the ImageNet Large-Scale Visual Recognition Challenge 2015. Code is publicly available at https://github.com/myfavouritekk/T-CNN.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}} for {{Video Technology}}},
  keywords = {_st/Cited,_st/Skimmed,computer vision,Convolutional codes,neural networks,Neural networks,Object detection,Proposals,Target tracking,Training,Videos},
  file = {/Users/maksimploter/Zotero/storage/CHRGRNR9/Kang et al. - 2018 - T-CNN Tubelets With Convolutional Neural Networks for Object Detection From Videos.pdf;/Users/maksimploter/Zotero/storage/B8ZQA63K/8003302.html}
}

@inproceedings{kangObjectDetectionVideo2016,
  title = {Object {{Detection}} from {{Video Tubelets}} with {{Convolutional Neural Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kang, Kai and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
  date = {2016-06},
  eprint = {1604.04053},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {817--825},
  doi = {10.1109/CVPR.2016.95},
  url = {http://arxiv.org/abs/1604.04053},
  urldate = {2025-03-27},
  abstract = {Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (R-CNN)). The lately introduced ImageNet task on object detection from video (VID) brings the object detection task into the video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task.},
  keywords = {_r/3_worth_citing,_r/4_should_cite,_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition,video object detection},
  file = {/Users/maksimploter/Zotero/storage/67B2K9SH/Kang et al. - 2016 - Object Detection from Video Tubelets with Convolutional Neural Networks.pdf;/Users/maksimploter/Zotero/storage/LKSEAZWN/1604.html}
}
@InProceedings{Lu_2017_ICCV,
author = {Lu, Yongyi and Lu, Cewu and Tang, Chi-Keung},
title = { Online Video Object Detection Using Association LSTM},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}
@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation},
  title={Long Short-Term Memory},
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}
}

@misc{xiaoVideoObjectDetection2018,
  title = {Video {{Object Detection}} with an {{Aligned Spatial-Temporal Memory}}},
  author = {Xiao, Fanyi and Lee, Yong Jae},
  year = {2018},
  month = jul,
  number = {arXiv:1712.06317},
  eprint = {1712.06317},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.06317},
  urldate = {2025-04-07},
  abstract = {We introduce Spatial-Temporal Memory Networks for video object detection. At its core, a novel Spatial-Temporal Memory module (STMM) serves as the recurrent computation unit to model long-term temporal appearance and motion dynamics. The STMM's design enables full integration of pretrained backbone CNN weights, which we find to be critical for accurate detection. Furthermore, in order to tackle object motion in videos, we propose a novel MatchTrans module to align the spatial-temporal memory from frame to frame. Our method produces state-of-the-art results on the benchmark ImageNet VID dataset, and our ablative studies clearly demonstrate the contribution of our different design choices. We release our code and models at http://fanyix.cs.ucdavis.edu/project/stmn/project.html.},
  archiveprefix = {arXiv},
  keywords = {_st/Skimmed,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/KK76QNMC/Xiao and Lee - 2018 - Video Object Detection with an Aligned Spatial-Temporal Memory.pdf;/Users/maksimploter/Zotero/storage/LRYHDLY5/1712.html}
}

@misc{ballasDelvingDeeperConvolutional2016,
  title = {Delving {{Deeper}} into {{Convolutional Networks}} for {{Learning Video Representations}}},
  author = {Ballas, Nicolas and Yao, Li and Pal, Chris and Courville, Aaron},
  year = {2016},
  month = mar,
  number = {arXiv:1511.06432},
  eprint = {1511.06432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06432},
  urldate = {2025-04-08},
  abstract = {We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call "percepts" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.},
  archiveprefix = {arXiv},
  keywords = {_r/4_should_cite,_st/New,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/maksimploter/Zotero/storage/8P8DALFN/Ballas et al. - 2016 - Delving Deeper into Convolutional Networks for Learning Video Representations.pdf;/Users/maksimploter/Zotero/storage/W3L9ZQBZ/1511.html}
}

@online{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  date = {2021-06-22},
  eprint = {2103.03206},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.03206},
  urldate = {2024-06-28},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domainspecific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver – a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/maksimploter/Zotero/storage/7SFRYG4J/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf}
}



%~~~~~~~~~~~~~~~~~~~METHOD~~~~~~~~~~~~~~~~~~~



@misc{srivastava2016unsupervisedlearningvideorepresentations,
      title={Unsupervised Learning of Video Representations using LSTMs}, 
      author={Nitish Srivastava and Elman Mansimov and Ruslan Salakhutdinov},
      year={2016},
      eprint={1502.04681},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.04681}, 
}

@article{everinghamPascalVisualObject2010,
  title = {The {{Pascal Visual Object Classes}} ({{VOC}}) {{Challenge}}},
  author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  date = {2010-06-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {88},
  number = {2},
  pages = {303--338},
  issn = {1573-1405},
  doi = {10.1007/s11263-009-0275-4},
  url = {https://doi.org/10.1007/s11263-009-0275-4},
  urldate = {2025-04-28},
  abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
  langid = {english},
  keywords = {_st/New,Artificial Intelligence,Benchmark,Database,Object detection,Object recognition},
  file = {/Users/maksimploter/Zotero/storage/GBFJDGPA/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf}
}

@online{linMicrosoftCOCOCommon2015a,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-21},
  eprint = {1405.0312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1405.0312},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2025-04-28},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  pubstate = {prepublished},
  keywords = {_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/8TYCVHII/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/Users/maksimploter/Zotero/storage/3NRGC7K3/1405.html}
}

@online{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  date = {2020-05-28},
  eprint = {2005.12872},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.12872},
  urldate = {2024-07-04},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/WEDNLTW2/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf}
}

@online{stewartEndtoendPeopleDetection2015,
  title = {End-to-End People Detection in Crowded Scenes},
  author = {Stewart, Russell and Andriluka, Mykhaylo},
  date = {2015-07-08},
  eprint = {1506.04878},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.04878},
  urldate = {2024-08-07},
  abstract = {Current people detectors operate either by scanning an image in a sliding window fashion or by classifying a discrete set of proposals. We propose a model that is based on decoding an image into a set of people detections. Our system takes an image as input and directly outputs a set of distinct detection hypotheses. Because we generate predictions jointly, common post-processing steps such as non-maximum suppression are unnecessary. We use a recurrent LSTM layer for sequence generation and train our model end-to-end with a new loss function that operates on sets of detections. We demonstrate the effectiveness of our approach on the challenging task of detecting people in crowded scenes.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/7EQFV3ER/Stewart and Andriluka - 2015 - End-to-end people detection in crowded scenes.pdf}
}

@online{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2018-02-07},
  eprint = {1708.02002},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02002},
  url = {http://arxiv.org/abs/1708.02002},
  urldate = {2025-01-16},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  pubstate = {prepublished},
  version = {2},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/W4JY7FFF/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf;/Users/maksimploter/Zotero/storage/2DLSHUN5/1708.html}
}

@online{rezatofighiGeneralizedIntersectionUnion2019,
  title = {Generalized {{Intersection}} over {{Union}}: {{A Metric}} and {{A Loss}} for {{Bounding Box Regression}}},
  shorttitle = {Generalized {{Intersection}} over {{Union}}},
  author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  date = {2019-04-15},
  eprint = {1902.09630},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.09630},
  url = {http://arxiv.org/abs/1902.09630},
  urldate = {2025-05-03},
  abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that \$IoU\$ can be directly used as a regression loss. However, \$IoU\$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of \$IoU\$ by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized \$IoU\$ (\$GIoU\$) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, \$IoU\$ based, and new, \$GIoU\$ based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/maksimploter/Zotero/storage/FSMDVCDN/Rezatofighi et al. - 2019 - Generalized Intersection over Union A Metric and A Loss for Bounding Box Regression.pdf;/Users/maksimploter/Zotero/storage/TSUZ8CH9/1902.html}
}

@online{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-05-09},
  eprint = {1506.02640},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2024-07-31},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/M5YCNGWW/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf}
}

@article{kuhnHungarianMethodAssignment1955,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  date = {1955},
  journaltitle = {Naval Research Logistics Quarterly},
  volume = {2},
  number = {1--2},
  pages = {83--97},
  issn = {1931-9193},
  doi = {10.1002/nav.3800020109},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
  urldate = {2025-05-03},
  abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
  langid = {english},
  keywords = {_st/New _st/Cited},
  file = {/Users/maksimploter/Zotero/storage/LG3XKK4H/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf;/Users/maksimploter/Zotero/storage/94HGMX8K/nav.html}
}



%~~~~~~~~~~~~~~~~~~~EXPERIMENTS~~~~~~~~~~~~~~~~~~~

@online{loshchilovDecoupledWeightDecay2019a,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2025-05-04},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/maksimploter/Zotero/storage/TIDCDZPV/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/maksimploter/Zotero/storage/QV42LIAQ/1711.html}
}

@online{redmonYOLO9000BetterFaster2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  shorttitle = {{{YOLO9000}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2016-12-25},
  eprint = {1612.08242},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.08242},
  url = {http://arxiv.org/abs/1612.08242},
  urldate = {2025-05-04},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  pubstate = {prepublished},
  keywords = {_st/Cited,_st/New,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/maksimploter/Zotero/storage/EI8XCJBE/Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf;/Users/maksimploter/Zotero/storage/Q3VCFWRI/1612.html}
}



@software{Jocher_Ultralytics_YOLO_2023,
author = {Jocher, Glenn and Qiu, Jing and Chaurasia, Ayush},
license = {AGPL-3.0},
month = jan,
title = {{Ultralytics YOLO}},
url = {https://github.com/ultralytics/ultralytics},
version = {8.0.0},
year = {2023}
}


@unpublished{tamara_munzner_keynote_2012,
	location = {Broad Institute},
	title = {Keynote on Visualization Principles},
	url = {https://www.youtube.com/watch?v=ZgOF8R6YL2U},
	abstract = {Copyright Broad Institute, 2013. All rights reserved.
Tamara Munzner (www.bit.ly/tmunzner) presents very lucid and useful guidelines for creating effective visualizations, including how to correctly rank visual channel types and how to use categorical color constraints. She explains advantages of 2D representation and drawbacks of 3D, immersive, or animated visualizations. She also describes how to create visualizations that reduce the viewer's cognitive load, and how to validate visualizations. This talk was presented at {VIZBI} 2011, an international conference series on visualizing biological data (vizbi.org) funded by {NIH} \& {EMBO}. Slides from the talk are at bit.ly/{nCJM}5U.

For information about data visualization efforts at the Broad Institute, please visit:
http://www.broadinstitute.org/node/1363/},
	author = {Tamara Munzner},
	urldate = {2024-12-05},
	date = {2012},
}

@book{graves_strategic_2012,
	edition = {2nd edition},
	title = {A Strategic Guide to Technical Communication},
	isbn = {978-1-55481-107-6},
	abstract = {A Strategic Guide to Technical Communication incorporates useful and specific strategies for writers, to enable them to create aesthetically appealing and usable technical documentation. These strategies have been developed and tested on a thousand students from a number of different disciplines over twelve years and three institutions. The second edition adds a chapter on business communication, reworks the discussion on technical style, and expands the information on visual communication and ethics into free-standing chapters.  The text is accompanied by a passcode-protected website containing materials for instructors ({PowerPoint} lectures, lesson plans, sample student work, and helpful links).},
	pagetotal = {328},
	publisher = {Broadview Press},
	author = {Graves, Heather and Graves, Roger},
	date = {2012-05-23},
}
